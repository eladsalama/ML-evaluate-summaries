{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceType":"datasetVersion","sourceId":2977194,"datasetId":1825054,"databundleVersionId":3024896},{"sourceType":"datasetVersion","sourceId":8558953,"datasetId":5115575,"databundleVersionId":8703624},{"sourceType":"datasetVersion","sourceId":8925262,"datasetId":5307916,"databundleVersionId":9087307}],"dockerImageVersionId":30732,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"# General imports\nimport numpy as np\nimport random\nimport shutil\nimport os\n\n# Neural network imports\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras import layers\nfrom transformers import TFAutoModel, AutoTokenizer\n!pip install /kaggle/input/autocorrect/autocorrect-2.6.1.tar\nfrom autocorrect import Speller\n\n# Initializing autocorrect\nspell = Speller(lang='en', fast=True)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-28T20:06:31.618759Z","iopub.execute_input":"2024-07-28T20:06:31.619945Z","iopub.status.idle":"2024-07-28T20:07:32.558484Z","shell.execute_reply.started":"2024-07-28T20:06:31.619878Z","shell.execute_reply":"2024-07-28T20:07:32.557066Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-07-28 20:06:34.037223: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-28 20:06:34.037396: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-28 20:06:34.226219: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Processing /kaggle/input/autocorrect/autocorrect-2.6.1.tar\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hBuilding wheels for collected packages: autocorrect\n  Building wheel for autocorrect (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for autocorrect: filename=autocorrect-2.6.1-py3-none-any.whl size=622364 sha256=f291815204d2a636de42508081b0248e8f01b7500249530b0bdbf54b2a6e777e\n  Stored in directory: /root/.cache/pip/wheels/db/69/42/0fb0421d2fe70d195a04665edc760cfe5fd341d7bb8d8e0aaa\nSuccessfully built autocorrect\nInstalling collected packages: autocorrect\nSuccessfully installed autocorrect-2.6.1\n","output_type":"stream"}]},{"cell_type":"code","source":"class CFG:\n    epochs=12\n    pre_trained_model_name=\"/kaggle/input/deberta-v3-large/deberta_v3_large/\"\n    final_model_path = f'full_model_scaled-{epochs}.keras'\n    learning_rate=0.00015\n    weight_decay=1e-4\n    warmup_steps=100\n    hidden_dropout_prob=0.\n    attention_probs_dropout_prob=0.\n    n_splits=4\n    batch_size=4\n    random_seed=42\n    max_length=1575\n    embeddings_len=1024","metadata":{"execution":{"iopub.status.busy":"2024-07-28T20:07:32.561110Z","iopub.execute_input":"2024-07-28T20:07:32.562347Z","iopub.status.idle":"2024-07-28T20:07:32.570226Z","shell.execute_reply.started":"2024-07-28T20:07:32.562301Z","shell.execute_reply":"2024-07-28T20:07:32.568872Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def seed_everything(random_seed):\n    \n    os.environ['PYTHONHASHSEED'] = str(random_seed)\n    np.random.seed(random_seed)\n    tf.random.set_seed(random_seed)\n    random.seed(random_seed)\n    keras.utils.set_random_seed(random_seed)","metadata":{"execution":{"iopub.status.busy":"2024-07-28T20:07:32.571868Z","iopub.execute_input":"2024-07-28T20:07:32.572292Z","iopub.status.idle":"2024-07-28T20:07:32.586191Z","shell.execute_reply.started":"2024-07-28T20:07:32.572258Z","shell.execute_reply":"2024-07-28T20:07:32.584988Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def move_to_working_folder(source_path, destination_path):\n    shutil.copy(source_path, destination_path)","metadata":{"execution":{"iopub.status.busy":"2024-07-28T20:07:32.588901Z","iopub.execute_input":"2024-07-28T20:07:32.589407Z","iopub.status.idle":"2024-07-28T20:07:32.598721Z","shell.execute_reply.started":"2024-07-28T20:07:32.589375Z","shell.execute_reply":"2024-07-28T20:07:32.597496Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Preprocess Input","metadata":{}},{"cell_type":"code","source":"prefix1 = \"Think through this step by step: \"\nprefix2 = \"Pay attention to the content and wording: \"\n\n# This function creates input ids, attention mask and head mask\ndef preprocess(summary, prompt_question, prompt_text, tokenizer, is_demo=False):\n    \n    sep = f\" {tokenizer.sep_token} \" \n    \n    if is_demo:\n        summary = prefix1 + prompt_question + sep + prefix2 + spell(summary) + sep + prompt_text\n        tokenized = tokenizer(summary,\n                              add_special_tokens=False,\n                              truncation=True,\n                              padding='max_length',\n                              return_tensors='tf',\n                              max_length=CFG.max_length,\n                              return_attention_mask=True)\n    else:\n        summary = prefix1 + prompt_question + sep + prefix2 + summary.apply(spell) + sep + prompt_text\n        tokenized = tokenizer.batch_encode_plus(summary.tolist(),\n                                                add_special_tokens=False,\n                                                truncation=True,\n                                                padding='max_length',\n                                                return_tensors='tf',\n                                                max_length=CFG.max_length,\n                                                return_attention_mask=True)\n    \n    input_ids = tokenized['input_ids']\n    attention_mask = tokenized['attention_mask']\n\n    # Create head mask\n    head_mask = np.zeros(input_ids.shape)\n    for i, summ in enumerate(input_ids.numpy()):\n        use_full = False\n        for j, token in enumerate(summ):\n            if token == tokenizer.sep_token_id:\n                use_full = not use_full  \n            elif token == tokenizer.pad_token_id:\n                break\n            head_mask[i][j] = (1. if use_full else 0.) \n    return [input_ids.numpy(), attention_mask.numpy(), head_mask.astype(np.float16)]","metadata":{"execution":{"iopub.status.busy":"2024-07-28T20:07:32.600328Z","iopub.execute_input":"2024-07-28T20:07:32.601047Z","iopub.status.idle":"2024-07-28T20:07:32.617258Z","shell.execute_reply.started":"2024-07-28T20:07:32.601013Z","shell.execute_reply":"2024-07-28T20:07:32.615993Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Define Model","metadata":{}},{"cell_type":"code","source":"# Creates a model that wraps the pre trained model\n@keras.utils.register_keras_serializable()\nclass PreTrainedModel(keras.Model):\n    def __init__(self, model_path, trainable=False, num_layers_to_freeze=0, name=None, **kwargs):\n        super().__init__(name=name, **kwargs)\n        self.model_path = model_path\n        self.trainable = trainable\n        self.num_layers_to_freeze = num_layers_to_freeze\n        \n        # Load model and tokenizer\n        self.model = TFAutoModel.from_pretrained(model_path + \"model\") \n        self.tokenizer = AutoTokenizer.from_pretrained(model_path + \"tokenizer\")\n        \n        # Define model configurations\n        self.model.trainable = self.trainable\n        self.model.config.hidden_dropout_prob = CFG.hidden_dropout_prob\n        self.model.config.attention_probs_dropout_prob = CFG.attention_probs_dropout_prob\n        \n        # Freeze layers if trainable\n        if self.trainable:\n            self.model.trainable = self.trainable\n            if self.trainable:\n                for layer in self.model.layers[0].encoder.layer[:self.num_layers_to_freeze]:\n                    layer.trainable = False\n\n    # Call the pre trained model and get the all hidden state\n    def call(self, input_ids, attention_mask):\n        output = self.model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)\n        return output.hidden_states\n    \n    def get_config(self):\n        config = super().get_config().copy()\n        config.update({\n            'model_path': self.model_path,\n            'trainable': self.trainable,\n            'num_layers_to_freeze': self.num_layers_to_freeze\n        })\n        return config\n\n    @classmethod\n    def from_config(cls, config):\n        return cls(**config)\n    \n@keras.utils.register_keras_serializable()\ndef build_deberta():\n    return PreTrainedModel(CFG.pre_trained_model_name, name=\"deberta_layer\")","metadata":{"execution":{"iopub.status.busy":"2024-07-28T20:07:32.618792Z","iopub.execute_input":"2024-07-28T20:07:32.619151Z","iopub.status.idle":"2024-07-28T20:07:32.671766Z","shell.execute_reply.started":"2024-07-28T20:07:32.619097Z","shell.execute_reply":"2024-07-28T20:07:32.670730Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Define layers for head mask step\n\n@keras.utils.register_keras_serializable()\nclass ExpandDimsLayer(layers.Layer):\n    def __init__(self, **kwargs):\n        super(ExpandDimsLayer, self).__init__(**kwargs)\n\n    def call(self, inputs):\n        return tf.expand_dims(tf.cast(inputs, dtype=tf.float32), axis=-1)\n\n@keras.utils.register_keras_serializable()\nclass MaskedEmbeddingsLayer(layers.Layer):\n    def __init__(self, **kwargs):\n        super(MaskedEmbeddingsLayer, self).__init__(**kwargs)\n\n    def call(self, inputs):\n        hidden_state, h_mask = inputs\n        return tf.multiply(hidden_state, h_mask)","metadata":{"execution":{"iopub.status.busy":"2024-07-28T20:07:32.673473Z","iopub.execute_input":"2024-07-28T20:07:32.673917Z","iopub.status.idle":"2024-07-28T20:07:32.684480Z","shell.execute_reply.started":"2024-07-28T20:07:32.673865Z","shell.execute_reply":"2024-07-28T20:07:32.683015Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### Define Loss Function","metadata":{}},{"cell_type":"code","source":"# The loss function\n@keras.utils.register_keras_serializable()\ndef mcrmse(y_true, y_pred):\n    y_true = tf.cast(y_true, tf.float16)\n    y_pred = tf.cast(y_pred, tf.float16)\n    columnwise_mse = tf.reduce_mean(tf.square(y_true - y_pred), axis=0)\n    return tf.reduce_mean(tf.sqrt(columnwise_mse), axis=-1)","metadata":{"execution":{"iopub.status.busy":"2024-07-28T20:07:32.685943Z","iopub.execute_input":"2024-07-28T20:07:32.686335Z","iopub.status.idle":"2024-07-28T20:07:32.701088Z","shell.execute_reply.started":"2024-07-28T20:07:32.686294Z","shell.execute_reply":"2024-07-28T20:07:32.699783Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# Generate Predictions","metadata":{}},{"cell_type":"code","source":"def generate_predictions(model, data):\n    contents = []\n    wordings = []\n    ids = []\n    predictions = model.predict(x=[data['input_ids'], data['attention_mask'], data['head_mask']],\n                                batch_size=CFG.batch_size)\n\n    for idx, output in enumerate(predictions):\n        contents.append(output[0])\n        wordings.append(output[1])\n        ids.append(data['student_id'][idx])\n\n    contents = np.exp(contents) - 3\n    wordings = np.exp(wordings) - 3\n        \n    return ids, contents, wordings","metadata":{"execution":{"iopub.status.busy":"2024-07-28T20:07:32.702649Z","iopub.execute_input":"2024-07-28T20:07:32.703031Z","iopub.status.idle":"2024-07-28T20:07:32.712831Z","shell.execute_reply.started":"2024-07-28T20:07:32.702996Z","shell.execute_reply":"2024-07-28T20:07:32.711666Z"},"trusted":true},"execution_count":9,"outputs":[]}]}