{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec528014",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a436c5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pro_paths = [r\"C:\\Users\\salam\\Documents\\GitHub\", \"\"]\n",
    "train_pro_path = train_pro_paths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40425ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPLORING THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3569d705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>prompt_question</th>\n",
       "      <th>prompt_title</th>\n",
       "      <th>prompt_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39c16e</td>\n",
       "      <td>Summarize at least 3 elements of an ideal trag...</td>\n",
       "      <td>On Tragedy</td>\n",
       "      <td>Chapter 13 \\r\\nAs the sequel to what has alrea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3b9047</td>\n",
       "      <td>In complete sentences, summarize the structure...</td>\n",
       "      <td>Egyptian Social Structure</td>\n",
       "      <td>Egyptian society was structured like a pyramid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>814d6b</td>\n",
       "      <td>Summarize how the Third Wave developed over su...</td>\n",
       "      <td>The Third Wave</td>\n",
       "      <td>Background \\r\\nThe Third Wave experiment took ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ebad26</td>\n",
       "      <td>Summarize the various ways the factory would u...</td>\n",
       "      <td>Excerpt from The Jungle</td>\n",
       "      <td>With one member trimming beef in a cannery, an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  prompt_id                                    prompt_question  \\\n",
       "0    39c16e  Summarize at least 3 elements of an ideal trag...   \n",
       "1    3b9047  In complete sentences, summarize the structure...   \n",
       "2    814d6b  Summarize how the Third Wave developed over su...   \n",
       "3    ebad26  Summarize the various ways the factory would u...   \n",
       "\n",
       "                prompt_title  \\\n",
       "0                 On Tragedy   \n",
       "1  Egyptian Social Structure   \n",
       "2             The Third Wave   \n",
       "3    Excerpt from The Jungle   \n",
       "\n",
       "                                         prompt_text  \n",
       "0  Chapter 13 \\r\\nAs the sequel to what has alrea...  \n",
       "1  Egyptian society was structured like a pyramid...  \n",
       "2  Background \\r\\nThe Third Wave experiment took ...  \n",
       "3  With one member trimming beef in a cannery, an...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pro = pd.read_csv(train_pro_path + \"\\ML-evaluate-summaries\\data\\prompts_train.csv\")\n",
    "train_pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f754b82",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>student_id</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>text</th>\n",
       "      <th>content</th>\n",
       "      <th>wording</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000e8c3c7ddb</td>\n",
       "      <td>814d6b</td>\n",
       "      <td>The third wave was an experimentto see how peo...</td>\n",
       "      <td>0.205683</td>\n",
       "      <td>0.380538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0020ae56ffbf</td>\n",
       "      <td>ebad26</td>\n",
       "      <td>They would rub it up with soda to make the sme...</td>\n",
       "      <td>-0.548304</td>\n",
       "      <td>0.506755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>004e978e639e</td>\n",
       "      <td>3b9047</td>\n",
       "      <td>In Egypt, there were many occupations and soci...</td>\n",
       "      <td>3.128928</td>\n",
       "      <td>4.231226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>005ab0199905</td>\n",
       "      <td>3b9047</td>\n",
       "      <td>The highest class was Pharaohs these people we...</td>\n",
       "      <td>-0.210614</td>\n",
       "      <td>-0.471415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0070c9e7af47</td>\n",
       "      <td>814d6b</td>\n",
       "      <td>The Third Wave developed  rapidly because the ...</td>\n",
       "      <td>3.272894</td>\n",
       "      <td>3.219757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7160</th>\n",
       "      <td>ff7c7e70df07</td>\n",
       "      <td>ebad26</td>\n",
       "      <td>They used all sorts of chemical concoctions to...</td>\n",
       "      <td>0.205683</td>\n",
       "      <td>0.380538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7161</th>\n",
       "      <td>ffc34d056498</td>\n",
       "      <td>3b9047</td>\n",
       "      <td>The lowest classes are slaves and farmers slav...</td>\n",
       "      <td>-0.308448</td>\n",
       "      <td>0.048171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7162</th>\n",
       "      <td>ffd1576d2e1b</td>\n",
       "      <td>3b9047</td>\n",
       "      <td>they sorta made people start workin...</td>\n",
       "      <td>-1.408180</td>\n",
       "      <td>-0.493603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7163</th>\n",
       "      <td>ffe4a98093b2</td>\n",
       "      <td>39c16e</td>\n",
       "      <td>An ideal tragety has three elements that make ...</td>\n",
       "      <td>-0.393310</td>\n",
       "      <td>0.627128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7164</th>\n",
       "      <td>fffbccfd8a08</td>\n",
       "      <td>ebad26</td>\n",
       "      <td>The meat would smell sour but the would \"rub i...</td>\n",
       "      <td>1.771596</td>\n",
       "      <td>0.547742</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7165 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        student_id prompt_id  \\\n",
       "0     000e8c3c7ddb    814d6b   \n",
       "1     0020ae56ffbf    ebad26   \n",
       "2     004e978e639e    3b9047   \n",
       "3     005ab0199905    3b9047   \n",
       "4     0070c9e7af47    814d6b   \n",
       "...            ...       ...   \n",
       "7160  ff7c7e70df07    ebad26   \n",
       "7161  ffc34d056498    3b9047   \n",
       "7162  ffd1576d2e1b    3b9047   \n",
       "7163  ffe4a98093b2    39c16e   \n",
       "7164  fffbccfd8a08    ebad26   \n",
       "\n",
       "                                                   text   content   wording  \n",
       "0     The third wave was an experimentto see how peo...  0.205683  0.380538  \n",
       "1     They would rub it up with soda to make the sme... -0.548304  0.506755  \n",
       "2     In Egypt, there were many occupations and soci...  3.128928  4.231226  \n",
       "3     The highest class was Pharaohs these people we... -0.210614 -0.471415  \n",
       "4     The Third Wave developed  rapidly because the ...  3.272894  3.219757  \n",
       "...                                                 ...       ...       ...  \n",
       "7160  They used all sorts of chemical concoctions to...  0.205683  0.380538  \n",
       "7161  The lowest classes are slaves and farmers slav... -0.308448  0.048171  \n",
       "7162             they sorta made people start workin... -1.408180 -0.493603  \n",
       "7163  An ideal tragety has three elements that make ... -0.393310  0.627128  \n",
       "7164  The meat would smell sour but the would \"rub i...  1.771596  0.547742  \n",
       "\n",
       "[7165 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sum = pd.read_csv(train_pro_path + \"\\ML-evaluate-summaries\\data\\summaries_train.csv\")\n",
    "train_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3ea730b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>student_id</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000000ffffff</td>\n",
       "      <td>abc123</td>\n",
       "      <td>Example text 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>111111eeeeee</td>\n",
       "      <td>def789</td>\n",
       "      <td>Example text 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>222222cccccc</td>\n",
       "      <td>abc123</td>\n",
       "      <td>Example text 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>333333dddddd</td>\n",
       "      <td>def789</td>\n",
       "      <td>Example text 4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     student_id prompt_id            text\n",
       "0  000000ffffff    abc123  Example text 1\n",
       "1  111111eeeeee    def789  Example text 2\n",
       "2  222222cccccc    abc123  Example text 3\n",
       "3  333333dddddd    def789  Example text 4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sum = pd.read_csv(train_pro_path + \"\\ML-evaluate-summaries\\data\\summaries_test.csv\")\n",
    "test_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06eed977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>prompt_question</th>\n",
       "      <th>prompt_title</th>\n",
       "      <th>prompt_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abc123</td>\n",
       "      <td>Summarize...</td>\n",
       "      <td>Example Title 1</td>\n",
       "      <td>Heading\\nText...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>def789</td>\n",
       "      <td>Summarize...</td>\n",
       "      <td>Example Title 2</td>\n",
       "      <td>Heading\\nText...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  prompt_id prompt_question     prompt_title       prompt_text\n",
       "0    abc123    Summarize...  Example Title 1  Heading\\nText...\n",
       "1    def789    Summarize...  Example Title 2  Heading\\nText..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pro = pd.read_csv(train_pro_path + \"\\ML-evaluate-summaries\\data\\prompts_test.csv\")\n",
    "test_pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d059d634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will need to take a portion of the train data as test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da3b2e34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>prompt_question</th>\n",
       "      <th>prompt_title</th>\n",
       "      <th>prompt_text</th>\n",
       "      <th>student_id</th>\n",
       "      <th>text</th>\n",
       "      <th>content</th>\n",
       "      <th>wording</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39c16e</td>\n",
       "      <td>Summarize at least 3 elements of an ideal trag...</td>\n",
       "      <td>On Tragedy</td>\n",
       "      <td>Chapter 13 \\r\\nAs the sequel to what has alrea...</td>\n",
       "      <td>00791789cc1f</td>\n",
       "      <td>1 element of an ideal tragedy is that it shoul...</td>\n",
       "      <td>-0.210614</td>\n",
       "      <td>-0.471415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39c16e</td>\n",
       "      <td>Summarize at least 3 elements of an ideal trag...</td>\n",
       "      <td>On Tragedy</td>\n",
       "      <td>Chapter 13 \\r\\nAs the sequel to what has alrea...</td>\n",
       "      <td>0086ef22de8f</td>\n",
       "      <td>The three elements of an ideal tragedy are:  H...</td>\n",
       "      <td>-0.970237</td>\n",
       "      <td>-0.417058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39c16e</td>\n",
       "      <td>Summarize at least 3 elements of an ideal trag...</td>\n",
       "      <td>On Tragedy</td>\n",
       "      <td>Chapter 13 \\r\\nAs the sequel to what has alrea...</td>\n",
       "      <td>0094589c7a22</td>\n",
       "      <td>Aristotle states that an ideal tragedy should ...</td>\n",
       "      <td>-0.387791</td>\n",
       "      <td>-0.584181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39c16e</td>\n",
       "      <td>Summarize at least 3 elements of an ideal trag...</td>\n",
       "      <td>On Tragedy</td>\n",
       "      <td>Chapter 13 \\r\\nAs the sequel to what has alrea...</td>\n",
       "      <td>00cd5736026a</td>\n",
       "      <td>One element of an Ideal tragedy is having a co...</td>\n",
       "      <td>0.088882</td>\n",
       "      <td>-0.594710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39c16e</td>\n",
       "      <td>Summarize at least 3 elements of an ideal trag...</td>\n",
       "      <td>On Tragedy</td>\n",
       "      <td>Chapter 13 \\r\\nAs the sequel to what has alrea...</td>\n",
       "      <td>00d98b8ff756</td>\n",
       "      <td>The 3 ideal of tragedy is how complex you need...</td>\n",
       "      <td>-0.687288</td>\n",
       "      <td>-0.460886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7160</th>\n",
       "      <td>ebad26</td>\n",
       "      <td>Summarize the various ways the factory would u...</td>\n",
       "      <td>Excerpt from The Jungle</td>\n",
       "      <td>With one member trimming beef in a cannery, an...</td>\n",
       "      <td>ff37545b2805</td>\n",
       "      <td>In paragraph two, they would use pickle meat a...</td>\n",
       "      <td>1.520355</td>\n",
       "      <td>-0.292990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7161</th>\n",
       "      <td>ebad26</td>\n",
       "      <td>Summarize the various ways the factory would u...</td>\n",
       "      <td>Excerpt from The Jungle</td>\n",
       "      <td>With one member trimming beef in a cannery, an...</td>\n",
       "      <td>ff4ed38ef099</td>\n",
       "      <td>in the first paragraph  it says \"either can it...</td>\n",
       "      <td>-1.204574</td>\n",
       "      <td>-1.169784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7162</th>\n",
       "      <td>ebad26</td>\n",
       "      <td>Summarize the various ways the factory would u...</td>\n",
       "      <td>Excerpt from The Jungle</td>\n",
       "      <td>With one member trimming beef in a cannery, an...</td>\n",
       "      <td>ff53b94f7ce0</td>\n",
       "      <td>They would have piles of filthy meat on the fl...</td>\n",
       "      <td>0.328739</td>\n",
       "      <td>-1.053294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7163</th>\n",
       "      <td>ebad26</td>\n",
       "      <td>Summarize the various ways the factory would u...</td>\n",
       "      <td>Excerpt from The Jungle</td>\n",
       "      <td>With one member trimming beef in a cannery, an...</td>\n",
       "      <td>ff7c7e70df07</td>\n",
       "      <td>They used all sorts of chemical concoctions to...</td>\n",
       "      <td>0.205683</td>\n",
       "      <td>0.380538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7164</th>\n",
       "      <td>ebad26</td>\n",
       "      <td>Summarize the various ways the factory would u...</td>\n",
       "      <td>Excerpt from The Jungle</td>\n",
       "      <td>With one member trimming beef in a cannery, an...</td>\n",
       "      <td>fffbccfd8a08</td>\n",
       "      <td>The meat would smell sour but the would \"rub i...</td>\n",
       "      <td>1.771596</td>\n",
       "      <td>0.547742</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7165 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     prompt_id                                    prompt_question  \\\n",
       "0       39c16e  Summarize at least 3 elements of an ideal trag...   \n",
       "1       39c16e  Summarize at least 3 elements of an ideal trag...   \n",
       "2       39c16e  Summarize at least 3 elements of an ideal trag...   \n",
       "3       39c16e  Summarize at least 3 elements of an ideal trag...   \n",
       "4       39c16e  Summarize at least 3 elements of an ideal trag...   \n",
       "...        ...                                                ...   \n",
       "7160    ebad26  Summarize the various ways the factory would u...   \n",
       "7161    ebad26  Summarize the various ways the factory would u...   \n",
       "7162    ebad26  Summarize the various ways the factory would u...   \n",
       "7163    ebad26  Summarize the various ways the factory would u...   \n",
       "7164    ebad26  Summarize the various ways the factory would u...   \n",
       "\n",
       "                 prompt_title  \\\n",
       "0                  On Tragedy   \n",
       "1                  On Tragedy   \n",
       "2                  On Tragedy   \n",
       "3                  On Tragedy   \n",
       "4                  On Tragedy   \n",
       "...                       ...   \n",
       "7160  Excerpt from The Jungle   \n",
       "7161  Excerpt from The Jungle   \n",
       "7162  Excerpt from The Jungle   \n",
       "7163  Excerpt from The Jungle   \n",
       "7164  Excerpt from The Jungle   \n",
       "\n",
       "                                            prompt_text    student_id  \\\n",
       "0     Chapter 13 \\r\\nAs the sequel to what has alrea...  00791789cc1f   \n",
       "1     Chapter 13 \\r\\nAs the sequel to what has alrea...  0086ef22de8f   \n",
       "2     Chapter 13 \\r\\nAs the sequel to what has alrea...  0094589c7a22   \n",
       "3     Chapter 13 \\r\\nAs the sequel to what has alrea...  00cd5736026a   \n",
       "4     Chapter 13 \\r\\nAs the sequel to what has alrea...  00d98b8ff756   \n",
       "...                                                 ...           ...   \n",
       "7160  With one member trimming beef in a cannery, an...  ff37545b2805   \n",
       "7161  With one member trimming beef in a cannery, an...  ff4ed38ef099   \n",
       "7162  With one member trimming beef in a cannery, an...  ff53b94f7ce0   \n",
       "7163  With one member trimming beef in a cannery, an...  ff7c7e70df07   \n",
       "7164  With one member trimming beef in a cannery, an...  fffbccfd8a08   \n",
       "\n",
       "                                                   text   content   wording  \n",
       "0     1 element of an ideal tragedy is that it shoul... -0.210614 -0.471415  \n",
       "1     The three elements of an ideal tragedy are:  H... -0.970237 -0.417058  \n",
       "2     Aristotle states that an ideal tragedy should ... -0.387791 -0.584181  \n",
       "3     One element of an Ideal tragedy is having a co...  0.088882 -0.594710  \n",
       "4     The 3 ideal of tragedy is how complex you need... -0.687288 -0.460886  \n",
       "...                                                 ...       ...       ...  \n",
       "7160  In paragraph two, they would use pickle meat a...  1.520355 -0.292990  \n",
       "7161  in the first paragraph  it says \"either can it... -1.204574 -1.169784  \n",
       "7162  They would have piles of filthy meat on the fl...  0.328739 -1.053294  \n",
       "7163  They used all sorts of chemical concoctions to...  0.205683  0.380538  \n",
       "7164  The meat would smell sour but the would \"rub i...  1.771596  0.547742  \n",
       "\n",
       "[7165 rows x 8 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train_pro.merge(train_sum , on = \"prompt_id\")\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4d6f2de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\salam\\miniconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import RobertaTokenizer, TFRobertaModel\n",
    "from tensorflow import keras\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f76224d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import logging\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8def7260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# disabling unnecceseray warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "logging.disable(logging.ERROR)\n",
    "#os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c67b0378",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Set random seeds\n",
    "random_seed = 42\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3294ca94",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "roberta = TFRobertaModel.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2c539ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing on a single entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60ecd5ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.7428983, -0.6235549]], dtype=float32)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = train['text'][0]\n",
    "tokens = tokenizer(input_text, return_tensors='tf')\n",
    "outputs = roberta(tokens)[0]\n",
    "last_hidden_state = np.array(outputs).mean(axis = 1)\n",
    "\n",
    "model = keras.Sequential([tf.keras.layers.Flatten(), \n",
    "                          tf.keras.layers.Dense(2, activation='linear')])\n",
    "\n",
    "dense_output = model(last_hidden_state)\n",
    "dense_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b6648ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (1, 768)                  0         \n",
      "                                                                 \n",
      " dense (Dense)               (1, 2)                    1538      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1538 (6.01 KB)\n",
      "Trainable params: 1538 (6.01 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7d06705d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\salam\\miniconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\salam\\miniconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\salam\\miniconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\salam\\miniconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\salam\\miniconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\salam\\AppData\\Local\\Temp\\__autograph_generated_fileumkis2zc.py\", line 40, in tf__run_call_with_unpacked_inputs\n        raise\n    File \"C:\\Users\\salam\\AppData\\Local\\Temp\\__autograph_generated_file22xajyxt.py\", line 31, in tf__call\n        outputs = ag__.converted_call(ag__.ld(self).roberta, (), dict(input_ids=ag__.ld(input_ids), attention_mask=ag__.ld(attention_mask), token_type_ids=ag__.ld(token_type_ids), position_ids=ag__.ld(position_ids), head_mask=ag__.ld(head_mask), inputs_embeds=ag__.ld(inputs_embeds), encoder_hidden_states=ag__.ld(encoder_hidden_states), encoder_attention_mask=ag__.ld(encoder_attention_mask), past_key_values=ag__.ld(past_key_values), use_cache=ag__.ld(use_cache), output_attentions=ag__.ld(output_attentions), output_hidden_states=ag__.ld(output_hidden_states), return_dict=ag__.ld(return_dict), training=ag__.ld(training)), fscope)\n    File \"C:\\Users\\salam\\AppData\\Local\\Temp\\__autograph_generated_fileumkis2zc.py\", line 40, in tf__run_call_with_unpacked_inputs\n        raise\n    File \"C:\\Users\\salam\\AppData\\Local\\Temp\\__autograph_generated_filepzbc2lpr.py\", line 76, in tf__call\n        batch_size, seq_length = ag__.ld(input_shape)\n\n    ValueError: Exception encountered when calling layer 'tf_roberta_model_1' (type TFRobertaModel).\n    \n    in user code:\n    \n        File \"C:\\Users\\salam\\miniconda3\\Lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1016, in run_call_with_unpacked_inputs  *\n            return func(self, **unpacked_inputs)\n        File \"C:\\Users\\salam\\miniconda3\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_tf_roberta.py\", line 1043, in call  *\n            outputs = self.roberta(\n        File \"C:\\Users\\salam\\miniconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"C:\\Users\\salam\\AppData\\Local\\Temp\\__autograph_generated_fileumkis2zc.py\", line 40, in tf__run_call_with_unpacked_inputs\n            raise\n        File \"C:\\Users\\salam\\AppData\\Local\\Temp\\__autograph_generated_filepzbc2lpr.py\", line 76, in tf__call\n            batch_size, seq_length = ag__.ld(input_shape)\n    \n        ValueError: Exception encountered when calling layer 'roberta' (type TFRobertaMainLayer).\n        \n        in user code:\n        \n            File \"C:\\Users\\salam\\miniconda3\\Lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1016, in run_call_with_unpacked_inputs  *\n                return func(self, **unpacked_inputs)\n            File \"C:\\Users\\salam\\miniconda3\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_tf_roberta.py\", line 739, in call  *\n                batch_size, seq_length = input_shape\n        \n            ValueError: not enough values to unpack (expected 2, got 1)\n        \n        \n        Call arguments received by layer 'roberta' (type TFRobertaMainLayer):\n          â€¢ input_ids=tf.Tensor(shape=(512,), dtype=int32)\n          â€¢ attention_mask=tf.Tensor(shape=(512,), dtype=int32)\n          â€¢ token_type_ids=None\n          â€¢ position_ids=None\n          â€¢ head_mask=None\n          â€¢ inputs_embeds=None\n          â€¢ encoder_hidden_states=None\n          â€¢ encoder_attention_mask=None\n          â€¢ past_key_values=None\n          â€¢ use_cache=True\n          â€¢ output_attentions=False\n          â€¢ output_hidden_states=False\n          â€¢ return_dict=True\n          â€¢ training=True\n    \n    \n    Call arguments received by layer 'tf_roberta_model_1' (type TFRobertaModel):\n      â€¢ input_ids={'input_ids': 'tf.Tensor(shape=(512,), dtype=int32)', 'attention_mask': 'tf.Tensor(shape=(512,), dtype=int32)'}\n      â€¢ attention_mask=None\n      â€¢ token_type_ids=None\n      â€¢ position_ids=None\n      â€¢ head_mask=None\n      â€¢ inputs_embeds=None\n      â€¢ encoder_hidden_states=None\n      â€¢ encoder_attention_mask=None\n      â€¢ past_key_values=None\n      â€¢ use_cache=None\n      â€¢ output_attentions=None\n      â€¢ output_hidden_states=None\n      â€¢ return_dict=None\n      â€¢ training=True\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(train_df, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\miniconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file79b5jx3l.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileumkis2zc.py:37\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__run_call_with_unpacked_inputs\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     36\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(func), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m),), \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(unpacked_inputs)), fscope)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file22xajyxt.py:31\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[0;32m     29\u001b[0m do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     30\u001b[0m retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[1;32m---> 31\u001b[0m outputs \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mroberta, (), \u001b[38;5;28mdict\u001b[39m(input_ids\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(input_ids), attention_mask\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(attention_mask), token_type_ids\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(token_type_ids), position_ids\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(position_ids), head_mask\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(head_mask), inputs_embeds\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(inputs_embeds), encoder_hidden_states\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(encoder_hidden_states), encoder_attention_mask\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(encoder_attention_mask), past_key_values\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(past_key_values), use_cache\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(use_cache), output_attentions\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(output_attentions), output_hidden_states\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(output_hidden_states), return_dict\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(return_dict), training\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(training)), fscope)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     33\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileumkis2zc.py:37\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__run_call_with_unpacked_inputs\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     36\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(func), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m),), \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(unpacked_inputs)), fscope)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filepzbc2lpr.py:76\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[0;32m     74\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_shape\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     75\u001b[0m ag__\u001b[38;5;241m.\u001b[39mif_stmt(ag__\u001b[38;5;241m.\u001b[39mand_(\u001b[38;5;28;01mlambda\u001b[39;00m: ag__\u001b[38;5;241m.\u001b[39mld(input_ids) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mlambda\u001b[39;00m: ag__\u001b[38;5;241m.\u001b[39mld(inputs_embeds) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m), if_body_3, else_body_3, get_state_3, set_state_3, (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_shape\u001b[39m\u001b[38;5;124m'\u001b[39m,), \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 76\u001b[0m batch_size, seq_length \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(input_shape)\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_state_4\u001b[39m():\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (past_key_values, past_key_values_length)\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\salam\\miniconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\salam\\miniconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\salam\\miniconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\salam\\miniconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\salam\\miniconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\salam\\AppData\\Local\\Temp\\__autograph_generated_fileumkis2zc.py\", line 40, in tf__run_call_with_unpacked_inputs\n        raise\n    File \"C:\\Users\\salam\\AppData\\Local\\Temp\\__autograph_generated_file22xajyxt.py\", line 31, in tf__call\n        outputs = ag__.converted_call(ag__.ld(self).roberta, (), dict(input_ids=ag__.ld(input_ids), attention_mask=ag__.ld(attention_mask), token_type_ids=ag__.ld(token_type_ids), position_ids=ag__.ld(position_ids), head_mask=ag__.ld(head_mask), inputs_embeds=ag__.ld(inputs_embeds), encoder_hidden_states=ag__.ld(encoder_hidden_states), encoder_attention_mask=ag__.ld(encoder_attention_mask), past_key_values=ag__.ld(past_key_values), use_cache=ag__.ld(use_cache), output_attentions=ag__.ld(output_attentions), output_hidden_states=ag__.ld(output_hidden_states), return_dict=ag__.ld(return_dict), training=ag__.ld(training)), fscope)\n    File \"C:\\Users\\salam\\AppData\\Local\\Temp\\__autograph_generated_fileumkis2zc.py\", line 40, in tf__run_call_with_unpacked_inputs\n        raise\n    File \"C:\\Users\\salam\\AppData\\Local\\Temp\\__autograph_generated_filepzbc2lpr.py\", line 76, in tf__call\n        batch_size, seq_length = ag__.ld(input_shape)\n\n    ValueError: Exception encountered when calling layer 'tf_roberta_model_1' (type TFRobertaModel).\n    \n    in user code:\n    \n        File \"C:\\Users\\salam\\miniconda3\\Lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1016, in run_call_with_unpacked_inputs  *\n            return func(self, **unpacked_inputs)\n        File \"C:\\Users\\salam\\miniconda3\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_tf_roberta.py\", line 1043, in call  *\n            outputs = self.roberta(\n        File \"C:\\Users\\salam\\miniconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"C:\\Users\\salam\\AppData\\Local\\Temp\\__autograph_generated_fileumkis2zc.py\", line 40, in tf__run_call_with_unpacked_inputs\n            raise\n        File \"C:\\Users\\salam\\AppData\\Local\\Temp\\__autograph_generated_filepzbc2lpr.py\", line 76, in tf__call\n            batch_size, seq_length = ag__.ld(input_shape)\n    \n        ValueError: Exception encountered when calling layer 'roberta' (type TFRobertaMainLayer).\n        \n        in user code:\n        \n            File \"C:\\Users\\salam\\miniconda3\\Lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1016, in run_call_with_unpacked_inputs  *\n                return func(self, **unpacked_inputs)\n            File \"C:\\Users\\salam\\miniconda3\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_tf_roberta.py\", line 739, in call  *\n                batch_size, seq_length = input_shape\n        \n            ValueError: not enough values to unpack (expected 2, got 1)\n        \n        \n        Call arguments received by layer 'roberta' (type TFRobertaMainLayer):\n          â€¢ input_ids=tf.Tensor(shape=(512,), dtype=int32)\n          â€¢ attention_mask=tf.Tensor(shape=(512,), dtype=int32)\n          â€¢ token_type_ids=None\n          â€¢ position_ids=None\n          â€¢ head_mask=None\n          â€¢ inputs_embeds=None\n          â€¢ encoder_hidden_states=None\n          â€¢ encoder_attention_mask=None\n          â€¢ past_key_values=None\n          â€¢ use_cache=True\n          â€¢ output_attentions=False\n          â€¢ output_hidden_states=False\n          â€¢ return_dict=True\n          â€¢ training=True\n    \n    \n    Call arguments received by layer 'tf_roberta_model_1' (type TFRobertaModel):\n      â€¢ input_ids={'input_ids': 'tf.Tensor(shape=(512,), dtype=int32)', 'attention_mask': 'tf.Tensor(shape=(512,), dtype=int32)'}\n      â€¢ attention_mask=None\n      â€¢ token_type_ids=None\n      â€¢ position_ids=None\n      â€¢ head_mask=None\n      â€¢ inputs_embeds=None\n      â€¢ encoder_hidden_states=None\n      â€¢ encoder_attention_mask=None\n      â€¢ past_key_values=None\n      â€¢ use_cache=None\n      â€¢ output_attentions=None\n      â€¢ output_hidden_states=None\n      â€¢ return_dict=None\n      â€¢ training=True\n"
     ]
    }
   ],
   "source": [
    "# Tokenize text data\n",
    "train_encodings = tokenizer.batch_encode_plus(train['text'].tolist(),\n",
    "                                              truncation=True, padding=True)\n",
    "\n",
    "# Create TensorFlow dataset\n",
    "train_df = tf.data.Dataset.from_tensor_slices(({'input_ids': train_encodings['input_ids'],\n",
    "                                                'attention_mask': train_encodings['attention_mask']},\n",
    "                                               {'content': train['content'].tolist(),\n",
    "                                                'wording': train['wording'].tolist()}))\n",
    "train_df = train_df.take(5)\n",
    "\n",
    "# Define Keras model\n",
    "model = keras.Sequential([roberta,\n",
    "                          tf.keras.layers.GlobalAveragePooling1D(),\n",
    "                          tf.keras.layers.Flatten(), \n",
    "                          tf.keras.layers.Dense(2, activation='linear')])\n",
    "\n",
    "# Compile the model (add loss, optimizer, metrics)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_df, epochs=5, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5c98aada",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This model has not yet been built. Build the model first by calling `build()` or by calling the model on a batch of data.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n",
      "File \u001b[1;32m~\\miniconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3506\u001b[0m, in \u001b[0;36mModel.summary\u001b[1;34m(self, line_length, positions, print_fn, expand_nested, show_trainable, layer_range)\u001b[0m\n\u001b[0;32m   3475\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Prints a string summary of the network.\u001b[39;00m\n\u001b[0;32m   3476\u001b[0m \n\u001b[0;32m   3477\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3503\u001b[0m \u001b[38;5;124;03m    ValueError: if `summary()` is called before the model is built.\u001b[39;00m\n\u001b[0;32m   3504\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3505\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilt:\n\u001b[1;32m-> 3506\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3507\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis model has not yet been built. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3508\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBuild the model first by calling `build()` or by calling \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3509\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe model on a batch of data.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3510\u001b[0m     )\n\u001b[0;32m   3511\u001b[0m layer_utils\u001b[38;5;241m.\u001b[39mprint_summary(\n\u001b[0;32m   3512\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   3513\u001b[0m     line_length\u001b[38;5;241m=\u001b[39mline_length,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3518\u001b[0m     layer_range\u001b[38;5;241m=\u001b[39mlayer_range,\n\u001b[0;32m   3519\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: This model has not yet been built. Build the model first by calling `build()` or by calling the model on a batch of data."
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "26c8d86b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tf.Tensor(\n",
      "[    0   134  7510     9    41  5631  6906    16    14    24   197    28\n",
      " 12121    15    10  2632   563     4  1437  2044  7510     9    41  5631\n",
      "  6906    16    14    24   197   129    33    65  1049   696     4    20\n",
      "    94  7510     9    41  5631  6906    16    14    24   197    33    10\n",
      "  1457 15019  6197     8    41  5483 21421    13   258   205     8  1099\n",
      "     4     2     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1], shape=(512,), dtype=int32)\n",
      "Attention mask: tf.Tensor(\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], shape=(512,), dtype=int32)\n",
      "Content: tf.Tensor(-0.21061394, shape=(), dtype=float32)\n",
      "Wording: tf.Tensor(-0.47141483, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for input_ids, attention_mask, content, wording in train_df:\n",
    "    print(\"Input IDs:\", input_ids)\n",
    "    print(\"Attention mask:\", attention_mask)\n",
    "    print(\"Content:\", content)\n",
    "    print(\"Wording:\", wording)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b70fed2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\salam\\miniconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\salam\\miniconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\salam\\miniconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\salam\\miniconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\salam\\miniconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\salam\\AppData\\Local\\Temp\\__autograph_generated_fileumkis2zc.py\", line 40, in tf__run_call_with_unpacked_inputs\n        raise\n    File \"C:\\Users\\salam\\AppData\\Local\\Temp\\__autograph_generated_file22xajyxt.py\", line 31, in tf__call\n        outputs = ag__.converted_call(ag__.ld(self).roberta, (), dict(input_ids=ag__.ld(input_ids), attention_mask=ag__.ld(attention_mask), token_type_ids=ag__.ld(token_type_ids), position_ids=ag__.ld(position_ids), head_mask=ag__.ld(head_mask), inputs_embeds=ag__.ld(inputs_embeds), encoder_hidden_states=ag__.ld(encoder_hidden_states), encoder_attention_mask=ag__.ld(encoder_attention_mask), past_key_values=ag__.ld(past_key_values), use_cache=ag__.ld(use_cache), output_attentions=ag__.ld(output_attentions), output_hidden_states=ag__.ld(output_hidden_states), return_dict=ag__.ld(return_dict), training=ag__.ld(training)), fscope)\n    File \"C:\\Users\\salam\\AppData\\Local\\Temp\\__autograph_generated_fileumkis2zc.py\", line 40, in tf__run_call_with_unpacked_inputs\n        raise\n    File \"C:\\Users\\salam\\AppData\\Local\\Temp\\__autograph_generated_filepzbc2lpr.py\", line 76, in tf__call\n        batch_size, seq_length = ag__.ld(input_shape)\n\n    ValueError: Exception encountered when calling layer 'tf_roberta_model_1' (type TFRobertaModel).\n    \n    in user code:\n    \n        File \"C:\\Users\\salam\\miniconda3\\Lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1016, in run_call_with_unpacked_inputs  *\n            return func(self, **unpacked_inputs)\n        File \"C:\\Users\\salam\\miniconda3\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_tf_roberta.py\", line 1043, in call  *\n            outputs = self.roberta(\n        File \"C:\\Users\\salam\\miniconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"C:\\Users\\salam\\AppData\\Local\\Temp\\__autograph_generated_fileumkis2zc.py\", line 40, in tf__run_call_with_unpacked_inputs\n            raise\n        File \"C:\\Users\\salam\\AppData\\Local\\Temp\\__autograph_generated_filepzbc2lpr.py\", line 76, in tf__call\n            batch_size, seq_length = ag__.ld(input_shape)\n    \n        ValueError: Exception encountered when calling layer 'roberta' (type TFRobertaMainLayer).\n        \n        in user code:\n        \n            File \"C:\\Users\\salam\\miniconda3\\Lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1016, in run_call_with_unpacked_inputs  *\n                return func(self, **unpacked_inputs)\n            File \"C:\\Users\\salam\\miniconda3\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_tf_roberta.py\", line 739, in call  *\n                batch_size, seq_length = input_shape\n        \n            ValueError: not enough values to unpack (expected 2, got 1)\n        \n        \n        Call arguments received by layer 'roberta' (type TFRobertaMainLayer):\n          â€¢ input_ids=tf.Tensor(shape=(512,), dtype=int32)\n          â€¢ attention_mask=tf.Tensor(shape=(512,), dtype=int32)\n          â€¢ token_type_ids=None\n          â€¢ position_ids=None\n          â€¢ head_mask=None\n          â€¢ inputs_embeds=None\n          â€¢ encoder_hidden_states=None\n          â€¢ encoder_attention_mask=None\n          â€¢ past_key_values=None\n          â€¢ use_cache=True\n          â€¢ output_attentions=False\n          â€¢ output_hidden_states=False\n          â€¢ return_dict=True\n          â€¢ training=True\n    \n    \n    Call arguments received by layer 'tf_roberta_model_1' (type TFRobertaModel):\n      â€¢ input_ids=tf.Tensor(shape=(512,), dtype=int32)\n      â€¢ attention_mask=tf.Tensor(shape=(512,), dtype=int32)\n      â€¢ token_type_ids=None\n      â€¢ position_ids=None\n      â€¢ head_mask=None\n      â€¢ inputs_embeds=None\n      â€¢ encoder_hidden_states=None\n      â€¢ encoder_attention_mask=None\n      â€¢ past_key_values=None\n      â€¢ use_cache=None\n      â€¢ output_attentions=None\n      â€¢ output_hidden_states=None\n      â€¢ return_dict=None\n      â€¢ training=True\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(train_df, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\miniconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file79b5jx3l.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileumkis2zc.py:37\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__run_call_with_unpacked_inputs\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     36\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(func), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m),), \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(unpacked_inputs)), fscope)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file22xajyxt.py:31\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[0;32m     29\u001b[0m do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     30\u001b[0m retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[1;32m---> 31\u001b[0m outputs \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mroberta, (), \u001b[38;5;28mdict\u001b[39m(input_ids\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(input_ids), attention_mask\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(attention_mask), token_type_ids\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(token_type_ids), position_ids\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(position_ids), head_mask\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(head_mask), inputs_embeds\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(inputs_embeds), encoder_hidden_states\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(encoder_hidden_states), encoder_attention_mask\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(encoder_attention_mask), past_key_values\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(past_key_values), use_cache\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(use_cache), output_attentions\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(output_attentions), output_hidden_states\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(output_hidden_states), return_dict\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(return_dict), training\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(training)), fscope)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     33\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileumkis2zc.py:37\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__run_call_with_unpacked_inputs\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     36\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(func), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m),), \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(unpacked_inputs)), fscope)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filepzbc2lpr.py:76\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[0;32m     74\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_shape\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     75\u001b[0m ag__\u001b[38;5;241m.\u001b[39mif_stmt(ag__\u001b[38;5;241m.\u001b[39mand_(\u001b[38;5;28;01mlambda\u001b[39;00m: ag__\u001b[38;5;241m.\u001b[39mld(input_ids) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mlambda\u001b[39;00m: ag__\u001b[38;5;241m.\u001b[39mld(inputs_embeds) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m), if_body_3, else_body_3, get_state_3, set_state_3, (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_shape\u001b[39m\u001b[38;5;124m'\u001b[39m,), \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 76\u001b[0m batch_size, seq_length \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(input_shape)\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_state_4\u001b[39m():\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (past_key_values, past_key_values_length)\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\salam\\miniconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\salam\\miniconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\salam\\miniconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\salam\\miniconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\salam\\miniconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\salam\\AppData\\Local\\Temp\\__autograph_generated_fileumkis2zc.py\", line 40, in tf__run_call_with_unpacked_inputs\n        raise\n    File \"C:\\Users\\salam\\AppData\\Local\\Temp\\__autograph_generated_file22xajyxt.py\", line 31, in tf__call\n        outputs = ag__.converted_call(ag__.ld(self).roberta, (), dict(input_ids=ag__.ld(input_ids), attention_mask=ag__.ld(attention_mask), token_type_ids=ag__.ld(token_type_ids), position_ids=ag__.ld(position_ids), head_mask=ag__.ld(head_mask), inputs_embeds=ag__.ld(inputs_embeds), encoder_hidden_states=ag__.ld(encoder_hidden_states), encoder_attention_mask=ag__.ld(encoder_attention_mask), past_key_values=ag__.ld(past_key_values), use_cache=ag__.ld(use_cache), output_attentions=ag__.ld(output_attentions), output_hidden_states=ag__.ld(output_hidden_states), return_dict=ag__.ld(return_dict), training=ag__.ld(training)), fscope)\n    File \"C:\\Users\\salam\\AppData\\Local\\Temp\\__autograph_generated_fileumkis2zc.py\", line 40, in tf__run_call_with_unpacked_inputs\n        raise\n    File \"C:\\Users\\salam\\AppData\\Local\\Temp\\__autograph_generated_filepzbc2lpr.py\", line 76, in tf__call\n        batch_size, seq_length = ag__.ld(input_shape)\n\n    ValueError: Exception encountered when calling layer 'tf_roberta_model_1' (type TFRobertaModel).\n    \n    in user code:\n    \n        File \"C:\\Users\\salam\\miniconda3\\Lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1016, in run_call_with_unpacked_inputs  *\n            return func(self, **unpacked_inputs)\n        File \"C:\\Users\\salam\\miniconda3\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_tf_roberta.py\", line 1043, in call  *\n            outputs = self.roberta(\n        File \"C:\\Users\\salam\\miniconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"C:\\Users\\salam\\AppData\\Local\\Temp\\__autograph_generated_fileumkis2zc.py\", line 40, in tf__run_call_with_unpacked_inputs\n            raise\n        File \"C:\\Users\\salam\\AppData\\Local\\Temp\\__autograph_generated_filepzbc2lpr.py\", line 76, in tf__call\n            batch_size, seq_length = ag__.ld(input_shape)\n    \n        ValueError: Exception encountered when calling layer 'roberta' (type TFRobertaMainLayer).\n        \n        in user code:\n        \n            File \"C:\\Users\\salam\\miniconda3\\Lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1016, in run_call_with_unpacked_inputs  *\n                return func(self, **unpacked_inputs)\n            File \"C:\\Users\\salam\\miniconda3\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_tf_roberta.py\", line 739, in call  *\n                batch_size, seq_length = input_shape\n        \n            ValueError: not enough values to unpack (expected 2, got 1)\n        \n        \n        Call arguments received by layer 'roberta' (type TFRobertaMainLayer):\n          â€¢ input_ids=tf.Tensor(shape=(512,), dtype=int32)\n          â€¢ attention_mask=tf.Tensor(shape=(512,), dtype=int32)\n          â€¢ token_type_ids=None\n          â€¢ position_ids=None\n          â€¢ head_mask=None\n          â€¢ inputs_embeds=None\n          â€¢ encoder_hidden_states=None\n          â€¢ encoder_attention_mask=None\n          â€¢ past_key_values=None\n          â€¢ use_cache=True\n          â€¢ output_attentions=False\n          â€¢ output_hidden_states=False\n          â€¢ return_dict=True\n          â€¢ training=True\n    \n    \n    Call arguments received by layer 'tf_roberta_model_1' (type TFRobertaModel):\n      â€¢ input_ids=tf.Tensor(shape=(512,), dtype=int32)\n      â€¢ attention_mask=tf.Tensor(shape=(512,), dtype=int32)\n      â€¢ token_type_ids=None\n      â€¢ position_ids=None\n      â€¢ head_mask=None\n      â€¢ inputs_embeds=None\n      â€¢ encoder_hidden_states=None\n      â€¢ encoder_attention_mask=None\n      â€¢ past_key_values=None\n      â€¢ use_cache=None\n      â€¢ output_attentions=None\n      â€¢ output_hidden_states=None\n      â€¢ return_dict=None\n      â€¢ training=True\n"
     ]
    }
   ],
   "source": [
    "# Tokenize text data\n",
    "train_encodings = tokenizer.batch_encode_plus(train['text'].tolist(),\n",
    "                                              truncation=True, padding=True)\n",
    "\n",
    "# Create TensorFlow dataset\n",
    "train_df = tf.data.Dataset.from_tensor_slices(({'input_ids': train_encodings['input_ids'],\n",
    "                                                'attention_mask': train_encodings['attention_mask']},\n",
    "                                               {'content': train['content'].tolist(),\n",
    "                                                'wording': train['wording'].tolist()}))\n",
    "train_df = train_df.take(5)\n",
    "\n",
    "# Define Keras model\n",
    "input_ids = tf.keras.Input(shape=(512,), dtype='int32', name='input_ids')\n",
    "attention_mask = tf.keras.Input(shape=(512,), dtype='int32', name='attention_mask')\n",
    "\n",
    "roberta_output = roberta(input_ids, attention_mask=attention_mask)[0]  # Accessing the output of the Roberta model\n",
    "\n",
    "# Flatten and dense layers\n",
    "flatten_output = tf.keras.layers.Flatten()(roberta_output)\n",
    "dense_output = tf.keras.layers.Dense(2, activation='linear')(flatten_output)\n",
    "\n",
    "model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=dense_output)\n",
    "\n",
    "# Compile the model (add loss, optimizer, metrics)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_df, epochs=5, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f347253",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "eedc6a92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7165"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cbeeaea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1 element of an ideal tragedy is that it shoul...\n",
       "1       The three elements of an ideal tragedy are:  H...\n",
       "2       Aristotle states that an ideal tragedy should ...\n",
       "3       One element of an Ideal tragedy is having a co...\n",
       "4       The 3 ideal of tragedy is how complex you need...\n",
       "                              ...                        \n",
       "7160    In paragraph two, they would use pickle meat a...\n",
       "7161    in the first paragraph  it says \"either can it...\n",
       "7162    They would have piles of filthy meat on the fl...\n",
       "7163    They used all sorts of chemical concoctions to...\n",
       "7164    The meat would smell sour but the would \"rub i...\n",
       "Name: text, Length: 7165, dtype: object"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3737779",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b54182",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df2fccd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bcf7c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4095eecd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
