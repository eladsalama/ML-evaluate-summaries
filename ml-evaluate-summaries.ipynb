{"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"sourceId":53482,"databundleVersionId":6201832,"sourceType":"competition"},{"sourceId":2977194,"sourceType":"datasetVersion","datasetId":1825054},{"sourceId":6258399,"sourceType":"datasetVersion","datasetId":3596984},{"sourceId":8139549,"sourceType":"datasetVersion","datasetId":4812209},{"sourceId":8139552,"sourceType":"datasetVersion","datasetId":4812212},{"sourceId":8141507,"sourceType":"datasetVersion","datasetId":4813598},{"sourceId":8146152,"sourceType":"datasetVersion","datasetId":4817190},{"sourceId":8469314,"sourceType":"datasetVersion","datasetId":5049859},{"sourceId":8507789,"sourceType":"datasetVersion","datasetId":5078374},{"sourceId":8558953,"sourceType":"datasetVersion","datasetId":5115575},{"sourceId":8565128,"sourceType":"datasetVersion","datasetId":5120491},{"sourceId":8583556,"sourceType":"datasetVersion","datasetId":5133543},{"sourceId":8642912,"sourceType":"datasetVersion","datasetId":5176303},{"sourceId":8642927,"sourceType":"datasetVersion","datasetId":5118268},{"sourceId":8656067,"sourceType":"datasetVersion","datasetId":5013476},{"sourceId":8771690,"sourceType":"datasetVersion","datasetId":5271375},{"sourceId":8794753,"sourceType":"datasetVersion","datasetId":5288118},{"sourceId":8802210,"sourceType":"datasetVersion","datasetId":5293469},{"sourceId":8805922,"sourceType":"datasetVersion","datasetId":5296131},{"sourceId":8809045,"sourceType":"datasetVersion","datasetId":5288639},{"sourceId":8811520,"sourceType":"datasetVersion","datasetId":5275294},{"sourceId":8812869,"sourceType":"datasetVersion","datasetId":5301169},{"sourceId":8818871,"sourceType":"datasetVersion","datasetId":5303542},{"sourceId":8826094,"sourceType":"datasetVersion","datasetId":5307916},{"sourceId":8827332,"sourceType":"datasetVersion","datasetId":5311005}],"dockerImageVersionId":30665,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":109.119668,"end_time":"2024-05-16T15:40:29.014408","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-05-16T15:38:39.894740","version":"2.5.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"059a11eda62448da981f7b2667b94637":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0d4e90cc3beb40b890ffd3978dfb22d8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"150f7c2a80124dad8408945c7c4a0fce":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"15104f9159d1410eb85b6379bc0756c1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ff9e6813dd3d499f8c44902f3806b49c","IPY_MODEL_588ef8f9110e4ba8897785a7816b52a6","IPY_MODEL_afa499e764d4423fbc616d6a8de52fd0"],"layout":"IPY_MODEL_deefef886f9c43cf88f6f162100f2e3e"}},"1982a9459b644ddf93b0670b17ed6427":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1b2c1961d94c4a07b162c8e1f1f4ea8f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1e281edfb4944800b70ca3e7b078cd13":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1f7d29516b7a46dcb0fa01415173e1df":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"25126573d458497987981721450a08ae":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2c8166010478471ebcfeaff488254cc3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2ed7fd040a494b91bcb6b11bf17446a7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3c55a6f3b2d6453d92734e1c2c6ccf3d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3f8ee3fcf5ed47f08eb3eeaff0647d5e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"42cdc6d0d53e4ab298128b8581bf4c3a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2c8166010478471ebcfeaff488254cc3","placeholder":"​","style":"IPY_MODEL_059a11eda62448da981f7b2667b94637","value":" 2.46M/2.46M [00:00&lt;00:00, 3.68MB/s]"}},"464d8379cbd842b6b0c919aa12c237ad":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1982a9459b644ddf93b0670b17ed6427","placeholder":"​","style":"IPY_MODEL_a0ed8c5740e846b7912490fe3541991c","value":"spm.model: 100%"}},"57d07143c3b54fb2baad23f96e0874aa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"588ef8f9110e4ba8897785a7816b52a6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0d4e90cc3beb40b890ffd3978dfb22d8","max":1736592160,"min":0,"orientation":"horizontal","style":"IPY_MODEL_150f7c2a80124dad8408945c7c4a0fce","value":1736592160}},"640eb7d806d7408ba3ba560fa99b9142":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"73ed4c4fc9104779b8fdb3fc52e64f4e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_aa4f2028d4be4adfb1389b240b2c7ee0","placeholder":"​","style":"IPY_MODEL_cd53dfa32e15440aadac97ed88dbec02","value":" 580/580 [00:00&lt;00:00, 52.0kB/s]"}},"76c12b9e7502419da6c5d8afae0b45ab":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8eb63820b62d49ea9bc8059798aabfb9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_464d8379cbd842b6b0c919aa12c237ad","IPY_MODEL_d006714a85f746a2a1485b6e1faf7003","IPY_MODEL_42cdc6d0d53e4ab298128b8581bf4c3a"],"layout":"IPY_MODEL_25126573d458497987981721450a08ae"}},"999bd4cca8c94c3a8c59da5cb58d8c2e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9ae4657f98844aa686e0a73f6bb69972":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_efc7d8e9f3cc457f93faf7367ab86cd3","IPY_MODEL_f37802e035ff44d181776f86eabe948d","IPY_MODEL_a915e75f5ed847a7b44823aadb68b8e3"],"layout":"IPY_MODEL_9cc123d4a6da4ac9bf37029593171ca6"}},"9cc123d4a6da4ac9bf37029593171ca6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9d8022fde15349cf92f52a24eac326ed":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a0ed8c5740e846b7912490fe3541991c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a915e75f5ed847a7b44823aadb68b8e3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_640eb7d806d7408ba3ba560fa99b9142","placeholder":"​","style":"IPY_MODEL_1e281edfb4944800b70ca3e7b078cd13","value":" 52.0/52.0 [00:00&lt;00:00, 4.37kB/s]"}},"aa1ceb8e11cc412ea3062b14dcb51319":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b3852559827e4748844d3b2927447926","max":580,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1b2c1961d94c4a07b162c8e1f1f4ea8f","value":580}},"aa4f2028d4be4adfb1389b240b2c7ee0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ad398d8a95ab459abdfa03879da117ba":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"afa499e764d4423fbc616d6a8de52fd0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e8b1ba7cf2354528ab658faf56c8985f","placeholder":"​","style":"IPY_MODEL_c4ef64c1429c4d049c4b952e1906bf5e","value":" 1.74G/1.74G [00:41&lt;00:00, 42.6MB/s]"}},"b3852559827e4748844d3b2927447926":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ba8f192523cf4f3d81fe72f0c6b16060":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c4ef64c1429c4d049c4b952e1906bf5e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cae13970055a44a09fbe31e19911087a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_76c12b9e7502419da6c5d8afae0b45ab","placeholder":"​","style":"IPY_MODEL_2ed7fd040a494b91bcb6b11bf17446a7","value":"config.json: 100%"}},"cd53dfa32e15440aadac97ed88dbec02":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cde68addf7004559a03c89e3c5cd5070":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d006714a85f746a2a1485b6e1faf7003":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ba8f192523cf4f3d81fe72f0c6b16060","max":2464616,"min":0,"orientation":"horizontal","style":"IPY_MODEL_57d07143c3b54fb2baad23f96e0874aa","value":2464616}},"deefef886f9c43cf88f6f162100f2e3e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e8b1ba7cf2354528ab658faf56c8985f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"efc7d8e9f3cc457f93faf7367ab86cd3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3f8ee3fcf5ed47f08eb3eeaff0647d5e","placeholder":"​","style":"IPY_MODEL_cde68addf7004559a03c89e3c5cd5070","value":"tokenizer_config.json: 100%"}},"f37802e035ff44d181776f86eabe948d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3c55a6f3b2d6453d92734e1c2c6ccf3d","max":52,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1f7d29516b7a46dcb0fa01415173e1df","value":52}},"f37f807ded5e4af5a607d3a76b4de99f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cae13970055a44a09fbe31e19911087a","IPY_MODEL_aa1ceb8e11cc412ea3062b14dcb51319","IPY_MODEL_73ed4c4fc9104779b8fdb3fc52e64f4e"],"layout":"IPY_MODEL_9d8022fde15349cf92f52a24eac326ed"}},"ff9e6813dd3d499f8c44902f3806b49c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_999bd4cca8c94c3a8c59da5cb58d8c2e","placeholder":"​","style":"IPY_MODEL_ad398d8a95ab459abdfa03879da117ba","value":"tf_model.h5: 100%"}}},"version_major":2,"version_minor":0}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"# General imports\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nimport matplotlib.pyplot as plt\nimport warnings\nimport datetime\nimport logging\nimport pickle\nimport random\nimport shutil\nimport os\nimport gc\nimport math\nimport spacy\nimport re\nimport string\n\n# Neural network imports\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras import layers\nfrom keras.callbacks import EarlyStopping\nfrom transformers import TFAutoModel, AutoTokenizer\nfrom sklearn.model_selection import GroupKFold\n!pip install /kaggle/input/autocorrect/autocorrect-2.6.1.tar\nfrom autocorrect import Speller\n\n# Lgbm imports\nimport lightgbm as lgb\nfrom lightgbm import LGBMRegressor, log_evaluation, early_stopping\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.metrics import mean_squared_error\nfrom typing import List\nfrom tqdm import tqdm\nimport json\nimport nltk\nfrom nltk.corpus import stopwords\nfrom collections import Counter\n# !pip install /kaggle/input/pyspellchecker/pyspellchecker-0.7.2-py3-none-any.whl\n# from spellchecker import SpellChecker\n# spellchecker = SpellChecker()\ntqdm.pandas()\n\n# disabling unnecceseray warnings\nwarnings.simplefilter(\"ignore\")\nlogging.disable(logging.ERROR)\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nkeras.mixed_precision.set_global_policy(\"mixed_float16\")\n# Set random seeds\nspell = Speller(lang='en', fast=True)","metadata":{"papermill":{"duration":33.079441,"end_time":"2024-05-16T15:39:15.664457","exception":false,"start_time":"2024-05-16T15:38:42.585016","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-01T12:30:40.940322Z","iopub.execute_input":"2024-07-01T12:30:40.940746Z","iopub.status.idle":"2024-07-01T12:31:41.483447Z","shell.execute_reply.started":"2024-07-01T12:30:40.940711Z","shell.execute_reply":"2024-07-01T12:31:41.482021Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-07-01 12:30:50.087868: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-01 12:30:50.088168: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-01 12:30:50.239134: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Processing /kaggle/input/autocorrect/autocorrect-2.6.1.tar\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hBuilding wheels for collected packages: autocorrect\n  Building wheel for autocorrect (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for autocorrect: filename=autocorrect-2.6.1-py3-none-any.whl size=622364 sha256=57e3eed19da61a834fedbae8c52a7f52fd863fe76c61dfcaad26bfb3371ed3f4\n  Stored in directory: /root/.cache/pip/wheels/db/69/42/0fb0421d2fe70d195a04665edc760cfe5fd341d7bb8d8e0aaa\nSuccessfully built autocorrect\nInstalling collected packages: autocorrect\nSuccessfully installed autocorrect-2.6.1\n","output_type":"stream"}]},{"cell_type":"code","source":"class CFG:\n    pre_trained_model_name=\"/kaggle/input/deberta-v3-large/deberta_v3_large/\"\n    final_model_path = '/full_model_scaled-6.keras'\n    learning_rate=0.00015\n    weight_decay=1e-4\n    warmup_steps=100\n    hidden_dropout_prob=0.\n    attention_probs_dropout_prob=0.\n    epochs=6\n    n_splits=4\n    batch_size=4\n    random_seed=42\n    max_length=1575\n    embeddings_len=1024","metadata":{"execution":{"iopub.status.busy":"2024-07-01T12:33:30.540393Z","iopub.execute_input":"2024-07-01T12:33:30.540694Z","iopub.status.idle":"2024-07-01T12:33:30.546716Z","shell.execute_reply.started":"2024-07-01T12:33:30.540655Z","shell.execute_reply":"2024-07-01T12:33:30.545654Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def seed_everything(random_seed):\n    \n    os.environ['PYTHONHASHSEED'] = str(random_seed)\n    np.random.seed(random_seed)\n    tf.random.set_seed(random_seed)\n    random.seed(random_seed)\n    keras.utils.set_random_seed(random_seed)\n    \nseed_everything(random_seed=CFG.random_seed)","metadata":{"execution":{"iopub.status.busy":"2024-07-01T12:31:51.505303Z","iopub.execute_input":"2024-07-01T12:31:51.505592Z","iopub.status.idle":"2024-07-01T12:31:51.512201Z","shell.execute_reply.started":"2024-07-01T12:31:51.505564Z","shell.execute_reply":"2024-07-01T12:31:51.511070Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def move_to_working_folder(source_path, destination_path):\n    shutil.copy(source_path, destination_path)","metadata":{"execution":{"iopub.status.busy":"2024-07-01T12:31:51.819531Z","iopub.execute_input":"2024-07-01T12:31:51.819822Z","iopub.status.idle":"2024-07-01T12:31:51.824876Z","shell.execute_reply.started":"2024-07-01T12:31:51.819792Z","shell.execute_reply":"2024-07-01T12:31:51.823828Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Load Data","metadata":{}},{"cell_type":"code","source":"data_path = '/kaggle/input/commonlit-evaluate-student-summaries/'\n\n# prompts train\ntrain_pro = pd.read_csv(data_path + 'prompts_train.csv')\n\n# summaries train\ntrain_sum = pd.read_csv(data_path + 'summaries_train.csv')\ntrain = train_pro.merge(train_sum , on = \"prompt_id\")\n\n# prompts test\ntest_pro = pd.read_csv(data_path + 'prompts_test.csv')\n\n# summaries test\ntest_sum = pd.read_csv(data_path + 'summaries_test.csv')\ntest = test_pro.merge(test_sum , on = \"prompt_id\")\ntest.head()","metadata":{"papermill":{"duration":0.175105,"end_time":"2024-05-16T15:39:15.917548","exception":false,"start_time":"2024-05-16T15:39:15.742443","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-01T12:31:53.351500Z","iopub.execute_input":"2024-07-01T12:31:53.351817Z","iopub.status.idle":"2024-07-01T12:31:53.515411Z","shell.execute_reply.started":"2024-07-01T12:31:53.351785Z","shell.execute_reply":"2024-07-01T12:31:53.514398Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"  prompt_id prompt_question     prompt_title       prompt_text    student_id  \\\n0    abc123    Summarize...  Example Title 1  Heading\\nText...  000000ffffff   \n1    abc123    Summarize...  Example Title 1  Heading\\nText...  222222cccccc   \n2    def789    Summarize...  Example Title 2  Heading\\nText...  111111eeeeee   \n3    def789    Summarize...  Example Title 2  Heading\\nText...  333333dddddd   \n\n             text  \n0  Example text 1  \n1  Example text 3  \n2  Example text 2  \n3  Example text 4  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>prompt_id</th>\n      <th>prompt_question</th>\n      <th>prompt_title</th>\n      <th>prompt_text</th>\n      <th>student_id</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>abc123</td>\n      <td>Summarize...</td>\n      <td>Example Title 1</td>\n      <td>Heading\\nText...</td>\n      <td>000000ffffff</td>\n      <td>Example text 1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>abc123</td>\n      <td>Summarize...</td>\n      <td>Example Title 1</td>\n      <td>Heading\\nText...</td>\n      <td>222222cccccc</td>\n      <td>Example text 3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>def789</td>\n      <td>Summarize...</td>\n      <td>Example Title 2</td>\n      <td>Heading\\nText...</td>\n      <td>111111eeeeee</td>\n      <td>Example text 2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>def789</td>\n      <td>Summarize...</td>\n      <td>Example Title 2</td>\n      <td>Heading\\nText...</td>\n      <td>333333dddddd</td>\n      <td>Example text 4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Data Exploration","metadata":{}},{"cell_type":"code","source":"# TODO\n\n# train['content'].hist(bins=20)\n# train.boxplot('content')\n# train['wording'].hist(bins=20)\n# train.boxplot('wording')","metadata":{"execution":{"iopub.status.busy":"2024-07-01T12:31:54.809274Z","iopub.execute_input":"2024-07-01T12:31:54.809551Z","iopub.status.idle":"2024-07-01T12:31:54.814718Z","shell.execute_reply.started":"2024-07-01T12:31:54.809524Z","shell.execute_reply":"2024-07-01T12:31:54.813418Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing (Full Model)","metadata":{}},{"cell_type":"code","source":"prefix1 = \"Think through this step by step: \"\nprefix2 = \"Pay attention to the content and wording: \"\n\n# This function creates input ids, attention mask, and head mask\ndef preprocess(summary, prompt_question, prompt_text, tokenizer):\n    \n    sep = f\" {tokenizer.sep_token} \" \n    summary = prefix1 + prompt_question + sep + prefix2 + summary.apply(spell) + sep + prompt_text\n    tokenized = tokenizer.batch_encode_plus(summary.tolist(),\n                                            add_special_tokens=False,\n                                            truncation=True,\n                                            padding='max_length',\n                                            return_tensors='tf',\n                                            max_length=CFG.max_length,\n                                            return_attention_mask=True)\n    \n    input_ids = tokenized['input_ids']\n    attention_mask = tokenized['attention_mask']\n\n    # Create head mask\n    head_mask = np.zeros(input_ids.shape)\n    for i, summ in enumerate(input_ids.numpy()):\n        use_full = False\n        for j, token in enumerate(summ):\n            if token == tokenizer.sep_token_id:\n                use_full = not use_full  \n            elif token == tokenizer.pad_token_id:\n                break\n            head_mask[i][j] = (1. if use_full else 0.) \n    \n    return [input_ids.numpy(), attention_mask.numpy(), head_mask]","metadata":{"execution":{"iopub.status.busy":"2024-07-01T12:31:56.051610Z","iopub.execute_input":"2024-07-01T12:31:56.051898Z","iopub.status.idle":"2024-07-01T12:31:56.060908Z","shell.execute_reply.started":"2024-07-01T12:31:56.051864Z","shell.execute_reply":"2024-07-01T12:31:56.059905Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# # Check head mask of first prompt (for debug)\n# # Run PreTrainedModel first\n\n# model = PreTrainedModel('/kaggle/input/deberta-v3-large/deberta_v3_large/')\n# sep = f\" {model.tokenizer.sep_token} \" \n# ids, mask, head = tokenize(train['text'], train['prompt_question'], train['prompt_text'], model.tokenizer)\n# train['input'] = prefix1 + train['prompt_question'] + sep + prefix2 + train['text'] # + sep + train['prompt_text']\n# first = model.tokenizer.tokenize(train['input'][500],                                               \n#           add_special_tokens=False,\n#           truncation=True,\n#           padding='max_length',\n#           return_tensors='tf',\n#           max_length=MAX_SUMMARY_LENGTH,\n#           return_attention_mask=False)\n\n# def find_indexes(array):\n#     return [index for index, value in enumerate(array) if value == 1]\n# np.array(first)[find_indexes(head[500])]","metadata":{"execution":{"iopub.status.busy":"2024-07-01T12:31:58.515602Z","iopub.execute_input":"2024-07-01T12:31:58.515892Z","iopub.status.idle":"2024-07-01T12:31:58.521321Z","shell.execute_reply.started":"2024-07-01T12:31:58.515861Z","shell.execute_reply":"2024-07-01T12:31:58.520091Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# Define Model","metadata":{}},{"cell_type":"code","source":"# Creates a model that wraps the pre trained model\n@keras.utils.register_keras_serializable()\nclass PreTrainedModel(keras.Model):\n    def __init__(self, model_path, trainable=False, num_layers_to_freeze=0, name=None, **kwargs):\n        super().__init__(name=name, **kwargs)\n        self.model_path = model_path\n        self.trainable = trainable\n        self.num_layers_to_freeze = num_layers_to_freeze\n        \n        # Load model and tokenizer\n        self.model = TFAutoModel.from_pretrained(model_path + \"model\") \n        self.tokenizer = AutoTokenizer.from_pretrained(model_path + \"tokenizer\")\n        \n        # Define model configurations\n        self.model.trainable = self.trainable\n        self.model.config.hidden_dropout_prob = CFG.hidden_dropout_prob\n        self.model.config.attention_probs_dropout_prob = CFG.attention_probs_dropout_prob\n        \n        # Freeze layers if trainable\n        if self.trainable:\n            self.model.trainable = self.trainable\n            if self.trainable:\n                for layer in self.model.layers[0].encoder.layer[:self.num_layers_to_freeze]:\n                    layer.trainable = False\n\n    # Call the pre trained model and get the all hidden state\n    def call(self, input_ids, attention_mask):\n        output = self.model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)\n        return output.hidden_states\n    \n    def get_config(self):\n        config = super().get_config().copy()\n        config.update({\n            'model_path': self.model_path,\n            'trainable': self.trainable,\n            'num_layers_to_freeze': self.num_layers_to_freeze\n        })\n        return config\n\n    @classmethod\n    def from_config(cls, config):\n        return cls(**config)","metadata":{"execution":{"iopub.status.busy":"2024-07-01T12:31:58.783398Z","iopub.execute_input":"2024-07-01T12:31:58.783661Z","iopub.status.idle":"2024-07-01T12:31:58.794040Z","shell.execute_reply.started":"2024-07-01T12:31:58.783635Z","shell.execute_reply":"2024-07-01T12:31:58.792804Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Define layers for head mask step\n\n@keras.utils.register_keras_serializable()\nclass ExpandDimsLayer(layers.Layer):\n    def __init__(self, **kwargs):\n        super(ExpandDimsLayer, self).__init__(**kwargs)\n\n    def call(self, inputs):\n        return tf.expand_dims(tf.cast(inputs, dtype=tf.float32), axis=-1)\n\n@keras.utils.register_keras_serializable()\nclass MaskedEmbeddingsLayer(layers.Layer):\n    def __init__(self, **kwargs):\n        super(MaskedEmbeddingsLayer, self).__init__(**kwargs)\n\n    def call(self, inputs):\n        hidden_state, h_mask = inputs\n        return tf.multiply(hidden_state, h_mask)","metadata":{"execution":{"iopub.status.busy":"2024-07-01T12:31:59.426484Z","iopub.execute_input":"2024-07-01T12:31:59.426799Z","iopub.status.idle":"2024-07-01T12:31:59.435424Z","shell.execute_reply.started":"2024-07-01T12:31:59.426765Z","shell.execute_reply":"2024-07-01T12:31:59.433957Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## Define loss function","metadata":{}},{"cell_type":"code","source":"# The loss function\n@keras.utils.register_keras_serializable()\ndef mcrmse(y_true, y_pred):\n    y_true = tf.cast(y_true, tf.float16)\n    y_pred = tf.cast(y_pred, tf.float16)\n    columnwise_mse = tf.reduce_mean(tf.square(y_true - y_pred), axis=0)\n    return tf.reduce_mean(tf.sqrt(columnwise_mse), axis=-1)","metadata":{"papermill":{"duration":0.032128,"end_time":"2024-05-16T15:40:24.400490","exception":false,"start_time":"2024-05-16T15:40:24.368362","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-01T12:32:00.110561Z","iopub.execute_input":"2024-07-01T12:32:00.110831Z","iopub.status.idle":"2024-07-01T12:32:00.116960Z","shell.execute_reply.started":"2024-07-01T12:32:00.110805Z","shell.execute_reply":"2024-07-01T12:32:00.115885Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"### (Optional) Add image of model design diagram","metadata":{}},{"cell_type":"code","source":"def build_deberta():\n    return PreTrainedModel(CFG.pre_trained_model_name, name=\"deberta_layer\")","metadata":{"execution":{"iopub.status.busy":"2024-07-01T12:32:01.277742Z","iopub.execute_input":"2024-07-01T12:32:01.278081Z","iopub.status.idle":"2024-07-01T12:32:01.284070Z","shell.execute_reply.started":"2024-07-01T12:32:01.278038Z","shell.execute_reply":"2024-07-01T12:32:01.282689Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def create_model(decay_steps=10000):\n    \n    # Instances\n    pre_trained_model_instance = build_deberta()\n    expand_dims_instance = ExpandDimsLayer(name='expand_dims')\n    mask_instance = MaskedEmbeddingsLayer(name='masked_embeddings')\n    avg_pooling_instance = layers.GlobalAveragePooling1D()\n    reshape_instance1 = layers.Reshape((1, -1), name='reshape_layer1')\n    reshape_instance2 = layers.Reshape((1, -1), name='reshape_layer2')\n    dense_instance = layers.Dense(embeddings_len, activation='gelu')\n\n    # The NN starts from here\n    \n    # Input layers\n    input_ids = keras.Input(shape=(CFG.max_length,), dtype='int32', name='input_ids')\n    attention_mask = keras.Input(shape=(CFG.max_length,), dtype='int32', name='attention_mask')\n    head_mask = keras.Input(shape=(CFG.max_length,), dtype='float32', name='head_mask')\n    \n    # Create embeddings and get all hidden states\n    hidden_states = pre_trained_model_instance(input_ids, attention_mask)\n    \n    # Mask pooling all hidden states of pre-trained model\n    pooled_hidden_states = []\n    for hidden_state in hidden_states:\n        h_mask = expand_dims_instance(head_mask)\n        masked_outputs = mask_instance([hidden_state, h_mask])\n        avg_pooling_layer = avg_pooling_instance(masked_outputs)\n        reshape_layer = reshape_instance1(avg_pooling_layer)\n        pooled_hidden_states.append(reshape_layer)\n    \n    # Concatenate all the hidden states an forward pass through LSTM\n    x = layers.Concatenate(axis=1)(pooled_hidden_states)\n    x = layers.LSTM(CFG.embeddings_len, return_sequences=False)(x)\n    \n    # Multi-sample Dropout\n    x = layers.Dropout(0.1)(x)\n    dropoutList = [reshape_instance2(dense_instance(layers.Dropout((i + 1) * 0.1)(x))) for i in range(5)]\n    x = layers.Concatenate(axis=1)(dropoutList)\n    x = layers.GlobalAveragePooling1D()(x)\n    \n    # Final dense layer\n    x = layers.Dense(512, activation='linear')(x)\n    x = layers.LayerNormalization()(x)\n    x = layers.Dropout(0.2)(x)\n    x = layers.Activation(keras.activations.gelu, name='gelu')(x)\n    \n    output_layer = layers.Dense(2, activation='linear')(x)\n    \n\n    # Compile model\n    model = keras.Model(inputs=[input_ids, attention_mask, head_mask], outputs=output_layer)\n    lr_schedule = tf.keras.optimizers.schedules.CosineDecay(initial_learning_rate=CFG.learning_rate,\n                                                            decay_steps=decay_steps,\n                                                            warmup_target=CFG.learning_rate,\n                                                            warmup_steps=CFG.warmup_steps,\n                                                           )\n    opt = keras.optimizers.AdamW(learning_rate=lr_schedule, weight_decay=CFG.weight_decay, use_ema=True)\n    model.compile(loss=mcrmse, optimizer=opt)\n    return model, pre_trained_model_instance","metadata":{"execution":{"iopub.status.busy":"2024-07-01T12:32:02.585830Z","iopub.execute_input":"2024-07-01T12:32:02.586794Z","iopub.status.idle":"2024-07-01T12:32:02.600064Z","shell.execute_reply.started":"2024-07-01T12:32:02.586752Z","shell.execute_reply":"2024-07-01T12:32:02.598886Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# Train Full Model","metadata":{}},{"cell_type":"code","source":"# Choose Training type\nTRAIN_WITH_FOLDS = False\nmove_to_working_folder('/kaggle/input/models/' + CFG.final_model_path, '/kaggle/working/' + CFG.final_model_path)","metadata":{"execution":{"iopub.status.busy":"2024-07-01T12:34:39.467679Z","iopub.execute_input":"2024-07-01T12:34:39.468840Z","iopub.status.idle":"2024-07-01T12:34:39.689972Z","shell.execute_reply.started":"2024-07-01T12:34:39.468791Z","shell.execute_reply":"2024-07-01T12:34:39.688953Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"X = train[['text', 'prompt_question', 'prompt_text']]    \n\n# apply transformations to the content and wording\ntrain['content_transformed'] = (train['content'] + 3).apply(np.log)\ntrain['wording_transformed'] = (train['wording'] + 3).apply(np.log)\ny = train[['content_transformed', 'wording_transformed']].astype('float16')\n\nif TRAIN_WITH_FOLDS:\n    # Train full model with GroupKFolds\n    gkf = GroupKFold(n_splits=4)\n    folds = gkf.split(X, y, groups=train['prompt_id'])\n\n    val_losses = []\n    histories = []\n\n    for i, (train_index, val_index) in enumerate(folds):\n        print(f\"Fold {i}\")\n        \n        X_train_fold, X_val_fold = X.iloc[train_index], X.iloc[val_index]\n        y_train_fold, y_val_fold = y.iloc[train_index], y.iloc[val_index]\n\n        decay_steps = math.ceil((len(X_train_fold) / CFG.batch_size) * CFG.epochs) \n        model, deberta = create_model(decay_steps=decay_steps)\n\n        X_train_fold = preprocess(X_train_fold['text'], X_train_fold['prompt_question'], X_train_fold['prompt_text'], deberta.tokenizer)\n        X_val_fold = preprocess(X_val_fold['text'], X_val_fold['prompt_question'], X_val_fold['prompt_text'], deberta.tokenizer)\n        \n        # Callbacks\n        early_stopping = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n        ema = keras.callbacks.SwapEMAWeights(swap_on_epoch=True)\n        ckptcb = keras.callbacks.ModelCheckpoint(\n            f\"full_model_fold_{i}\" + \".weights.h5\",\n            monitor=\"val_loss\",\n            save_best_only=True,\n            save_weights_only=True,\n            mode=\"min\",\n        ) \n\n        history = model.fit(x=X_train_fold,\n                            y=y_train_fold.values,\n                            validation_data=(X_val_fold, y_val_fold.values),\n                            epochs=epochs,\n                            batch_size=batch_size,\n                            callbacks=[early_stopping, ema, ckptcb],\n                            verbose=1)\n\n        # Get the validation loss from the last epoch\n        val_loss = min(history.history['val_loss'])\n        val_losses.append(val_loss)\n        histories.append(history)\n        print()\n\n    # Calculate the mean validation loss\n    mean_val_loss = np.mean(val_losses)\n    print(\"Mean Validation Loss:\", mean_val_loss)\n    \nelse:\n    # Train full model no folds all data\n    decay_steps = math.ceil((len(X) /  CFG.batch_size) * CFG.epochs) \n    path = '/kaggle/working/full_model_scaled-6.keras'\n    model = keras.models.load_model(path)\n    deberta = build_deberta()\n    \n    X = preprocess(X['text'], X['prompt_question'], X['prompt_text'], deberta.tokenizer)\n    \n    # Callbacks\n    ema = keras.callbacks.SwapEMAWeights(swap_on_epoch=True)\n    ckptcb = keras.callbacks.ModelCheckpoint(\n        \"full_model_scaled\" + \".keras\",\n        monitor=\"loss\",\n        save_best_only=True,\n        mode=\"min\",\n    )\n    \n    history = model.fit(x=X,\n                        y=y.values,\n                        epochs=CFG.epochs,\n                        batch_size=CFG.batch_size,\n                        callbacks=[ema, ckptcb],\n                        verbose=1)\n    \nprint('done')","metadata":{"execution":{"iopub.status.busy":"2024-07-01T12:19:50.559443Z","iopub.execute_input":"2024-07-01T12:19:50.559798Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/6\n\u001b[1m   1/1792\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m116:04:54\u001b[0m 233s/step - loss: 0.1655","output_type":"stream"}]},{"cell_type":"markdown","source":"# Inference ","metadata":{}},{"cell_type":"code","source":"def generate_predictions(model, data):\n    contents = []\n    wordings = []\n    ids = []\n    predictions = model.predict(x=[data['input_ids'], data['attention_mask'], data['head_mask']],\n                                batch_size=CFG.batch_size)\n\n    for idx, output in enumerate(predictions):\n        contents.append(output[0])\n        wordings.append(output[1])\n        ids.append(data['student_id'][idx])\n\n    contents = np.exp(contents) - 3\n    wordings = np.exp(wordings) - 3\n        \n    return ids, contents, wordings","metadata":{"execution":{"iopub.status.busy":"2024-07-01T12:44:37.252571Z","iopub.execute_input":"2024-07-01T12:44:37.252888Z","iopub.status.idle":"2024-07-01T12:44:37.261515Z","shell.execute_reply.started":"2024-07-01T12:44:37.252846Z","shell.execute_reply":"2024-07-01T12:44:37.260318Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"## Inference Baseline Model","metadata":{}},{"cell_type":"code","source":"# content_scores = np.random.uniform(-1.73, 3.9, len(test))\n# wording_scores = np.random.uniform(-1.96, 4.31, len(test))\n\n# submission_df = pd.DataFrame({'student_id': test['student_id'],\n#                               'content': content_scores,\n#                               'wording': wording_scores})\n\n# submission_df.to_csv(\"submission.csv\", index=False)\n# submission_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-29T15:17:04.599357Z","iopub.execute_input":"2024-06-29T15:17:04.599595Z","iopub.status.idle":"2024-06-29T15:17:04.611104Z","shell.execute_reply.started":"2024-06-29T15:17:04.599566Z","shell.execute_reply":"2024-06-29T15:17:04.610500Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## Inference Final Model","metadata":{}},{"cell_type":"code","source":"model_to_submit_path = '/kaggle/working/' + CFG.final_model_path\nmove_to_working_folder('/kaggle/input/models/' + CFG.final_model_path, model_to_submit_path)\n\n# Sort by prompt and text lengths\ntest['length'] = test['text'].apply(len) + test['prompt_text'].apply(len)\ntest = test.sort_values('length', ascending=True).reset_index(drop=True)\n\n\nX = test[['text', 'prompt_question', 'prompt_text']]\n\nmodel = keras.models.load_model(model_to_submit_path)\ndeberta = build_deberta()\n    \nX = preprocess(X['text'], X['prompt_question'], X['prompt_text'], deberta.tokenizer)\n\ntest_data = {\n    'input_ids': X[0],\n    'attention_mask': X[1],\n    'head_mask': X[2],\n    'student_id': test['student_id'],\n}\n\nmodel = keras.models.load_model(model_to_submit_path)\nids, contents, wordings = generate_predictions(model, test_data)\n    \n    \nsubmission_df = pd.DataFrame({'student_id': ids,\n                              'content': contents,\n                              'wording': wordings})\n\nsubmission_df.to_csv(\"submission.csv\", index=False)\nsubmission_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-01T12:44:39.554822Z","iopub.execute_input":"2024-07-01T12:44:39.555449Z","iopub.status.idle":"2024-07-01T12:49:22.558134Z","shell.execute_reply.started":"2024-07-01T12:44:39.555411Z","shell.execute_reply":"2024-07-01T12:49:22.557065Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 143s/step\n","output_type":"stream"},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"     student_id   content   wording\n0  000000ffffff -1.932617 -1.810547\n1  222222cccccc -1.935547 -1.812500\n2  111111eeeeee -1.937500 -1.821289\n3  333333dddddd -1.938477 -1.819336","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>student_id</th>\n      <th>content</th>\n      <th>wording</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>000000ffffff</td>\n      <td>-1.932617</td>\n      <td>-1.810547</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>222222cccccc</td>\n      <td>-1.935547</td>\n      <td>-1.812500</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>111111eeeeee</td>\n      <td>-1.937500</td>\n      <td>-1.821289</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>333333dddddd</td>\n      <td>-1.938477</td>\n      <td>-1.819336</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Leftovers","metadata":{}},{"cell_type":"code","source":"# **Results**\n\n# REAL RESULTS\n\n# baseline with grouped KFolds: 0.5310284495353699\n# From now on everything will be with grouped KFolds\n# EMA: 0.5303183048963547\n# EMA + lr_decay_steps=100000 + LSTM1=128 + LSTM2=64 + Dropout_after_LSTM2=0.6: 0.5258100032806396\n# EMA + lr_decay_steps=100000 + LSTM1=128 + LSTM2=32 + Dropout_after_LSTM2=0.4: 0.5223950520157814","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Augmentation Manipulation","metadata":{}},{"cell_type":"code","source":"# import nltk\n# from nltk.corpus import wordnet, stopwords\n# from googletrans import Translator\n# from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n\n# nltk.download('wordnet')\n# nltk.download('stopwords')\n\n# # Initialize the paraphrasing model\n# def synonym_replacement(words, n):\n#     new_words = words.copy() \n#     random_word_list = list(set([word for word in words if word not in stopwords.words('english')]))\n#     random.shuffle(random_word_list)\n#     num_replaced = 0\n#     for random_word in random_word_list:\n#         synonyms = get_synonyms(random_word)\n#         if len(synonyms) >= 1:\n#             synonym = random.choice(synonyms)\n#             new_words = [synonym if word == random_word else word for word in new_words]\n#             num_replaced += 1\n#         if num_replaced >= n: \n#             break\n#     return new_words\n\n# def get_synonyms(word):\n#     synonyms = set()\n#     for syn in wordnet.synsets(word):\n#         for lemma in syn.lemmas():\n#             synonyms.add(lemma.name())\n#     if word in synonyms:\n#         synonyms.remove(word)\n#     return list(synonyms)\n\n# def back_translation(text, src_lang='en', tgt_lang='fr'):\n#     try:\n#         translator = Translator()\n#         translated = translator.translate(text, src=src_lang, dest=tgt_lang).text\n#         back_translated = translator.translate(translated, src=tgt_lang, dest=src_lang).text\n#     except:\n#         print(f\"Error during back translation: {e}\")\n#         return text  # Return the original text in case of any error\n        \n#     return back_translated\n\n# def augment_text(text):\n#     augmentation_choice = random.choice(['synonym_replacement', 'back_translation'])\n#     words = text.split()\n    \n#     if augmentation_choice == 'synonym_replacement':\n#         augmented_text = ' '.join(synonym_replacement(words, n=250))\n#     elif augmentation_choice == 'back_translation':\n#         augmented_text = back_translation(text)\n#     elif augmentation_choice == 'paraphrase':\n#         augmented_text = paraphrase(text)\n    \n#     return augmented_text","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Meta Psuedo Labels Training Loop","metadata":{}},{"cell_type":"code","source":"# # loading augmented data\n# augmented_data = pd.read_excel('/kaggle/input/llm-generate-test/LLM_Generate_Test.xlsx')\n# augmented_data.columns = ['student_id', 'prompt_text', 'prompt_question', 'text']\n\n# def generate_predictions(model, data):\n#     contents = []\n#     wordings = []\n#     ids = []\n#     predictions = model.predict(x=[data['input_ids'], data['attention_mask'], data['head_mask']],\n#                                 batch_size=4)\n\n#     for idx, output in enumerate(predictions):\n#         contents.append(output[0])\n#         wordings.append(output[1])\n#         ids.append(data['student_id'][idx])\n#     return ids, contents, wordings\n\n# ROUNDS = 1\n\n# SAVED_DATASETS_INDEXES = []\n# SAVED_WEIGHTS_INDEXES = [0]\n\n# # The initial model\n# model, deberta = create_model()\n\n# if SAVED_WEIGHTS_INDEXES[-1] == 0:\n#     # Load weights of initial teacher\n#     model.load_weights('/kaggle/input/no-aug-full-model-4epochs/no_augment_full_model_4epochs..weights.h5')\n# else:\n#     # Load weights of last meta round\n#     print(f\"Loading weights: meta_model_{SAVED_WEIGHTS_INDEXES[-1]}\")\n#     model.load_weights(f'/kaggle/input/meta-model-weights/meta_model_{SAVED_WEIGHTS_INDEXES[-1]}.weights.h5')\n\n\n# # Tokenizing train data\n# X = tokenize(train['text'], train['prompt_question'], train['prompt_text'], deberta.tokenizer)\n# y = train[['content', 'wording']].astype('float16')\n\n# # Tokenizing augmented data\n# X_aug = tokenize(augmented_data['text'], augmented_data['prompt_quesetion'], augmented_data['prompt_text'], deberta.tokenizer)\n\n# aug_input = {\n#     'input_ids': X_aug[0],\n#     'attention_mask': X_aug[1],\n#     'head_mask': X_aug[2],\n#     'student_id': augmented_data['student_id']\n# }\n\n# for i in range(ROUNDS):\n            \n#     print(f\"Round {i}/{ROUNDS}\")\n    \n#     # Skip rounds we already done\n#     if i < SAVED_WEIGHTS_INDEXES[-1]:\n#         continue\n    \n#     # Predict meta psuedo labels\n    \n#     # Check if labeled augmentations ar already created\n#     if i in SAVED_DATASETS_INDEXES:\n#         # predictions already generated\n#         augmented_labeled_data = pd.read_csv(f'/kaggle/input/augmented-labeled-data/augmented_labeled_data_round_{i}.csv')\n#     else:\n#         print()\n#         print(f\"Generateing predictions...\")\n#         ids, contents, wordings = generate_predictions(model, aug_input)\n#         augmented_data['content'] = contents\n#         augmented_data['wording'] = wordings\n#         augmented_data.to_csv(f\"augmented_labeled_data_round_{i}.csv\")\n#         augmented_labeled_data = augmented_data\n        \n#     # update the labels    \n#     y_aug = augmented_labeled_data[['content', 'wording']].astype('float16')\n\n#     # checkpoint callback\n#     ckptcb = keras.callbacks.ModelCheckpoint(\n#         f\"meta_model_{i}\" + \".weights.h5\",\n#         monitor=\"loss\",\n#         save_best_only=True,\n#         save_weights_only=True,\n#         mode=\"min\",\n#     )\n    \n#     print()\n#     print(f\"Training on unlabeled data...\")\n#     model.fit(x=X_aug,\n#               y=y_aug.values,\n#               epochs=2,\n#               batch_size=batch_size,\n#               validation_data=(X, y.values),\n#               verbose=1)\n    \n#     # Fine tune the pre-trained model only on the labeled data\n#     print()\n#     print(f\"Training on labeled data...\")\n#     model.fit(x=X,\n#               y=y.values,\n#               epochs=2,\n#               batch_size=batch_size,\n#               callbacks=[ckptcb],\n#               verbose=1)\n#     print()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### LGBM + Feature Engineering (+6000)","metadata":{}},{"cell_type":"code","source":"# DATA_DIR = \"/kaggle/input/commonlit-evaluate-student-summaries/\"\n\n# prompts_train = pd.read_csv(DATA_DIR + \"prompts_train.csv\")\n# prompts_test = pd.read_csv(DATA_DIR + \"prompts_test.csv\")\n# summaries_train = pd.read_csv(DATA_DIR + \"summaries_train.csv\")\n# summaries_test = pd.read_csv(DATA_DIR + \"summaries_test.csv\")\n# sample_submission = pd.read_csv(DATA_DIR + \"sample_submission.csv\")\n\n\n# class Preprocessor:\n#     def __init__(self, model_name: str,) -> None:\n#         self.STOP_WORDS = set(stopwords.words('english'))\n\n#         self.tokenizer = AutoTokenizer.from_pretrained(model_name + \"tokenizer\")\n#         self.spacy_ner_model = spacy.load('en_core_web_sm',)\n#         self.speller = SpellChecker() #Speller(lang='en')\n        \n#     def count_text_length(self, df: pd.DataFrame, col:str) -> pd.Series:\n#         \"\"\" text length \"\"\"\n#         tokenizer=self.tokenizer\n#         return df[col].progress_apply(lambda x: len(tokenizer.encode(x)))\n\n#     def word_overlap_count(self, row):\n#         \"\"\" intersection(prompt_text, text) \"\"\"        \n#         def check_is_stop_word(word):\n#             return word in self.STOP_WORDS\n        \n#         prompt_words = row['prompt_tokens']\n#         summary_words = row['summary_tokens']\n#         if self.STOP_WORDS:\n#             prompt_words = list(filter(check_is_stop_word, prompt_words))\n#             summary_words = list(filter(check_is_stop_word, summary_words))\n#         return len(set(prompt_words).intersection(set(summary_words)))\n            \n#     def ngrams(self, token, n):\n#         # Use the zip function to help us generate n-grams\n#         # Concatentate the tokens into ngrams and return\n#         ngrams = zip(*[token[i:] for i in range(n)])\n#         return [\" \".join(ngram) for ngram in ngrams]\n\n#     def ngram_co_occurrence(self, row, n: int):\n#         # Tokenize the original text and summary into words\n#         original_tokens = row['prompt_tokens']\n#         summary_tokens = row['summary_tokens']\n\n#         # Generate n-grams for the original text and summary\n#         original_ngrams = set(self.ngrams(original_tokens, n))\n#         summary_ngrams = set(self.ngrams(summary_tokens, n))\n\n#         # Calculate the number of common n-grams\n#         common_ngrams = original_ngrams.intersection(summary_ngrams)\n\n#         # # Optionally, you can get the frequency of common n-grams for a more nuanced analysis\n#         # original_ngram_freq = Counter(ngrams(original_words, n))\n#         # summary_ngram_freq = Counter(ngrams(summary_words, n))\n#         # common_ngram_freq = {ngram: min(original_ngram_freq[ngram], summary_ngram_freq[ngram]) for ngram in common_ngrams}\n\n#         return len(common_ngrams)\n    \n#     def ner_overlap_count(self, row, mode:str):\n#         model = self.spacy_ner_model\n#         def clean_ners(ner_list):\n#             return set([(ner[0].lower(), ner[1]) for ner in ner_list])\n#         prompt = model(row['prompt_text'])\n#         summary = model(row['text'])\n\n#         if \"spacy\" in str(model):\n#             prompt_ner = set([(token.text, token.label_) for token in prompt.ents])\n#             summary_ner = set([(token.text, token.label_) for token in summary.ents])\n#         elif \"stanza\" in str(model):\n#             prompt_ner = set([(token.text, token.type) for token in prompt.ents])\n#             summary_ner = set([(token.text, token.type) for token in summary.ents])\n#         else:\n#             raise Exception(\"Model not supported\")\n\n#         prompt_ner = clean_ners(prompt_ner)\n#         summary_ner = clean_ners(summary_ner)\n\n#         intersecting_ners = prompt_ner.intersection(summary_ner)\n        \n#         ner_dict = dict(Counter([ner[1] for ner in intersecting_ners]))\n        \n#         if mode == \"train\":\n#             return ner_dict\n#         elif mode == \"test\":\n#             return {key: ner_dict.get(key) for key in self.ner_keys}\n\n    \n#     def quotes_count(self, row):\n#         summary = row['text']\n#         text = row['prompt_text']\n#         quotes_from_summary = re.findall(r'\"([^\"]*)\"', summary)\n#         if len(quotes_from_summary)>0:\n#             return [quote in text for quote in quotes_from_summary].count(True)\n#         else:\n#             return 0\n\n#     def spelling(self, text):\n        \n#         wordlist=text.split()\n#         amount_miss = len(list(self.speller.unknown(wordlist)))\n\n#         return amount_miss\n    \n#     def run(self, \n#             prompts: pd.DataFrame,\n#             summaries:pd.DataFrame,\n#             mode:str\n#         ) -> pd.DataFrame:\n        \n#         # before merge preprocess\n#         prompts[\"prompt_length\"] = prompts[\"prompt_text\"].apply(\n#             lambda x: len(self.tokenizer.encode(x))\n#         )\n#         prompts[\"prompt_tokens\"] = prompts[\"prompt_text\"].apply(\n#             lambda x: self.tokenizer.convert_ids_to_tokens(\n#                 self.tokenizer.encode(x), \n#                 skip_special_tokens=True\n#             )\n#         )\n\n#         summaries[\"summary_length\"] = summaries[\"text\"].apply(\n#             lambda x: len(self.tokenizer.encode(x))\n#         )\n#         summaries[\"summary_tokens\"] = summaries[\"text\"].apply(\n#             lambda x: self.tokenizer.convert_ids_to_tokens(\n#                 self.tokenizer.encode(x), \n#                 skip_special_tokens=True\n#             )\n\n#         )\n#         summaries[\"splling_err_num\"] = summaries[\"text\"].progress_apply(self.spelling)\n\n#         # merge prompts and summaries\n#         input_df = summaries.merge(prompts, how=\"left\", on=\"prompt_id\")\n\n#         # after merge preprocess\n#         input_df['length_ratio'] = input_df['summary_length'] / input_df['prompt_length']\n        \n#         input_df['word_overlap_count'] = input_df.progress_apply(self.word_overlap_count, axis=1)\n#         input_df['bigram_overlap_count'] = input_df.progress_apply(\n#             self.ngram_co_occurrence,args=(2,), axis=1 \n#         )\n#         input_df['trigram_overlap_count'] = input_df.progress_apply(\n#             self.ngram_co_occurrence, args=(3,), axis=1\n#         )\n        \n#         input_df['quotes_count'] = input_df.progress_apply(self.quotes_count, axis=1)\n        \n#         return input_df.drop(columns=[\"summary_tokens\", \"prompt_tokens\"])\n\n\n# model_path = '/kaggle/input/deberta-v3-large/deberta_v3_large/'\n# preprocessor = Preprocessor(model_name=model_path)\n\n# features_train = preprocessor.run(prompts_train, summaries_train, mode=\"train\")\n# features_test = preprocessor.run(prompts_test, summaries_test, mode=\"test\")\n# features_test['length'] = features_test['summary_length'] + features_test['prompt_length']\n# features_test = features_test.sort_values('length', ascending=True).reset_index(drop=True)\n# features_train.head()\n\n\n# data_path = '/kaggle/input/commonlit-evaluate-student-summaries/'\n# # prompts train\n# train_pro_polars = pl.read_csv(data_path + 'prompts_train.csv')\n# train_pro_polars.head(1)\n\n# # summaries train\n# train_sum_polars = pl.read_csv(data_path + 'summaries_train.csv')\n# train_sum_polars.head(1)\n\n# train_polars = train_pro_polars.join(train_sum_polars , on=\"prompt_id\", how='inner')\n# train_polars.head(1)\n\n# # prompts train\n# test_pro_polars = pl.read_csv(data_path + 'prompts_test.csv')\n# test_pro_polars.head(1)\n\n# # summaries train\n# test_sum_polars = pl.read_csv(data_path + 'summaries_test.csv')\n# test_sum_polars.head(1)\n\n# test_polars = test_pro_polars.join(test_sum_polars , on=\"prompt_id\", how='inner')\n# test_polars.head(1)\n\n# columns = [  \n#     (\n#         pl.col(\"text\").str.split(by=\"\\n\\n\").alias(\"paragraph\")\n#     ),\n# ]\n# # Load training and testing sets, while using \\ n \\ n character segmentation to list and renaming to paragraph for text data\n# train_polars = train_polars.with_columns(columns)\n# test_polars = test_polars.with_columns(columns)\n# # train_polars.head(1)\n\n# nlp = spacy.load(\"en_core_web_sm\")\n# with open('/kaggle/input/english-word-hx/words.txt', 'r') as file:\n#     english_vocab = set(word.strip().lower() for word in file)\n\n\n\n# # Feature Engineering\n\n# paragraph_fea = ['paragraph_len','paragraph_sentence_cnt','paragraph_word_cnt']\n# paragraph_fea2 = ['paragraph_error_num'] + paragraph_fea\n# sentence_fea = ['sentence_len','sentence_word_cnt']\n\n# def count_spelling_errors(text):\n#     doc = nlp(text)\n#     lemmatized_tokens = [token.lemma_.lower() for token in doc]\n#     spelling_errors = sum(1 for token in lemmatized_tokens if token not in english_vocab)\n#     return spelling_errors\n\n# def removeHTML(x):\n#     html=re.compile(r'<.*?>')\n#     return html.sub(r'',x)\n\n# def dataPreprocessing(x):\n#     # Convert words to lowercase\n#     x = x.lower()\n#     # Remove HTML\n#     x = removeHTML(x)\n#     # Delete strings starting with @\n#     x = re.sub(\"@\\w+\", '',x)\n#     # Delete Numbers\n#     x = re.sub(\"'\\d+\", '',x)\n#     x = re.sub(\"\\d+\", '',x)\n#     # Delete URL\n#     x = re.sub(\"http\\w+\", '',x)\n#     # Replace consecutive empty spaces with a single space character\n#     x = re.sub(r\"\\s+\", \" \", x)\n#     # Replace consecutive commas and periods with one comma and period character\n#     x = re.sub(r\"\\.+\", \".\", x)\n#     x = re.sub(r\"\\,+\", \",\", x)\n#     # Remove empty characters at the beginning and end\n#     x = x.strip()\n#     return x\n\n\n# # paragraph features\n# def remove_punctuation(text):\n#     \"\"\"\n#     Remove all punctuation from the input text.\n    \n#     Args:\n#     - text (str): The input text.\n    \n#     Returns:\n#     - str: The text with punctuation removed.\n#     \"\"\"\n#     # string.punctuation\n#     translator = str.maketrans('', '', string.punctuation)\n#     return text.translate(translator)\n\n\n# def Paragraph_Preprocess(tmp):\n#     # Expand the paragraph list into several lines of data\n#     tmp = tmp.explode('paragraph')\n#     # Paragraph preprocessing\n#     tmp = tmp.with_columns(pl.col('paragraph').map_elements(dataPreprocessing))\n#     tmp = tmp.with_columns(pl.col('paragraph').map_elements(remove_punctuation).alias('paragraph_no_pinctuation'))\n#     tmp = tmp.with_columns(pl.col('paragraph_no_pinctuation').map_elements(count_spelling_errors).alias(\"paragraph_error_num\"))\n#     # Calculate the length of each paragraph\n#     tmp = tmp.with_columns(pl.col('paragraph').map_elements(lambda x: len(x)).alias(\"paragraph_len\"))\n#     # Calculate the number of sentences and words in each paragraph\n#     tmp = tmp.with_columns(pl.col('paragraph').map_elements(lambda x: len(x.split('.'))).alias(\"paragraph_sentence_cnt\"),\n#                     pl.col('paragraph').map_elements(lambda x: len(x.split(' '))).alias(\"paragraph_word_cnt\"),)\n#     return tmp\n\n# def Paragraph_Eng(train_tmp):\n#     num_list = [0, 50,75,100,125,150,175,200,250,300,350,400,500,600]\n#     num_list2 = [0, 50,75,100,125,150,175,200,250,300,350,400,500,600,700]\n#     aggs = [\n#         # Count the number of paragraph lengths greater than and less than the i-value\n#         *[pl.col('paragraph').filter(pl.col('paragraph_len') >= i).count().alias(f\"paragraph_{i}_cnt\") for i in [0, 50,75,100,125,150,175,200,250,300,350,400,500,600,700] ], \n#         *[pl.col('paragraph').filter(pl.col('paragraph_len') <= i).count().alias(f\"paragraph_{i}_cnt\") for i in [25,49]], \n#         # other\n#         *[pl.col(fea).max().alias(f\"{fea}_max\") for fea in paragraph_fea2],\n#         *[pl.col(fea).mean().alias(f\"{fea}_mean\") for fea in paragraph_fea2],\n#         *[pl.col(fea).min().alias(f\"{fea}_min\") for fea in paragraph_fea2],\n#         *[pl.col(fea).sum().alias(f\"{fea}_sum\") for fea in paragraph_fea2],\n#         *[pl.col(fea).first().alias(f\"{fea}_first\") for fea in paragraph_fea2],\n#         *[pl.col(fea).last().alias(f\"{fea}_last\") for fea in paragraph_fea2],\n#         *[pl.col(fea).kurtosis().alias(f\"{fea}_kurtosis\") for fea in paragraph_fea2],\n#         *[pl.col(fea).quantile(0.25).alias(f\"{fea}_q1\") for fea in paragraph_fea2],  \n#         *[pl.col(fea).quantile(0.75).alias(f\"{fea}_q3\") for fea in paragraph_fea2],  \n#         ]\n    \n#     df = train_tmp.group_by(['student_id'], maintain_order=True).agg(aggs).sort(\"student_id\")\n#     df = df.to_pandas()\n#     return df\n\n# # sentence feature\n# def Sentence_Preprocess(tmp):\n#     # Preprocess text and use periods to segment sentences in the text\n#     tmp = tmp.with_columns(pl.col('text').map_elements(dataPreprocessing).str.split(by=\".\").alias(\"sentence\"))\n#     tmp = tmp.explode('sentence')\n#     # Calculate the length of a sentence\n#     tmp = tmp.with_columns(pl.col('sentence').map_elements(lambda x: len(x)).alias(\"sentence_len\"))\n#     # Filter out the portion of data with a sentence length greater than 15\n#     tmp = tmp.filter(pl.col('sentence_len')>=15)\n#     # Count the number of words in each sentence\n#     tmp = tmp.with_columns(pl.col('sentence').map_elements(lambda x: len(x.split(' '))).alias(\"sentence_word_cnt\"))\n    \n#     return tmp\n\n\n# # feature_eng\n# def Sentence_Eng(train_tmp):\n#     aggs = [\n#         # Count the number of sentences with a length greater than i\n#         *[pl.col('sentence').filter(pl.col('sentence_len') >= i).count().alias(f\"sentence_{i}_cnt\") for i in [0,15,50,100,150,200,250,300] ], \n#         *[pl.col('sentence').filter(pl.col('sentence_len') <= i).count().alias(f\"sentence_<{i}_cnt\") for i in [15,50] ], \n#         # other\n#         *[pl.col(fea).max().alias(f\"{fea}_max\") for fea in sentence_fea],\n#         *[pl.col(fea).mean().alias(f\"{fea}_mean\") for fea in sentence_fea],\n#         *[pl.col(fea).min().alias(f\"{fea}_min\") for fea in sentence_fea],\n#         *[pl.col(fea).sum().alias(f\"{fea}_sum\") for fea in sentence_fea],\n#         *[pl.col(fea).first().alias(f\"{fea}_first\") for fea in sentence_fea],\n#         *[pl.col(fea).last().alias(f\"{fea}_last\") for fea in sentence_fea],\n#         *[pl.col(fea).kurtosis().alias(f\"{fea}_kurtosis\") for fea in sentence_fea],\n#         *[pl.col(fea).quantile(0.25).alias(f\"{fea}_q1\") for fea in sentence_fea], \n#         *[pl.col(fea).quantile(0.75).alias(f\"{fea}_q3\") for fea in sentence_fea], \n#         ]\n#     df = train_tmp.group_by(['student_id'], maintain_order=True).agg(aggs).sort(\"student_id\")\n#     df = df.to_pandas()\n#     return df\n\n# # word feature\n# def Word_Preprocess(tmp):\n#     # Preprocess text and use spaces to separate words from the text\n#     tmp = tmp.with_columns(pl.col('text').map_elements(dataPreprocessing).str.split(by=\" \").alias(\"word\"))\n#     tmp = tmp.explode('word')\n#     # Calculate the length of each word\n#     tmp = tmp.with_columns(pl.col('word').map_elements(lambda x: len(x)).alias(\"word_len\"))\n#     # Delete data with a word length of 0\n#     tmp = tmp.filter(pl.col('word_len')!=0)\n    \n#     return tmp\n\n\n# # feature_eng\n# def Word_Eng(train_tmp):\n#     aggs = [\n#         # Count the number of words with a length greater than i+1\n#         *[pl.col('word').filter(pl.col('word_len') >= i+1).count().alias(f\"word_{i+1}_cnt\") for i in range(15) ], \n#         # other\n#         pl.col('word_len').max().alias(f\"word_len_max\"),\n#         pl.col('word_len').mean().alias(f\"word_len_mean\"),\n#         pl.col('word_len').std().alias(f\"word_len_std\"),\n#         pl.col('word_len').quantile(0.25).alias(f\"word_len_q1\"),\n#         pl.col('word_len').quantile(0.50).alias(f\"word_len_q2\"),\n#         pl.col('word_len').quantile(0.75).alias(f\"word_len_q3\"),\n#         ]\n#     df = train_tmp.group_by(['student_id'], maintain_order=True).agg(aggs).sort(\"student_id\")\n#     df = df.to_pandas()\n#     return df\n\n\n# # The preprocessing and feature engineering main function\n\n# def preprocess_data(data, fold_indexes=None, vectorizer=None, vectorizer_cnt=None):\n    \n#     tmp = Paragraph_Preprocess(data)\n#     train_feats = Paragraph_Eng(tmp)\n    \n#     # Obtain feature names\n#     feature_names = list(filter(lambda x: x not in ['student_id','content', 'wording'], train_feats.columns))\n#     print('Features Number: ',len(feature_names))\n    \n#     # Sentence features\n#     tmp = Sentence_Preprocess(data)\n#     # Merge the newly generated feature data with the previously generated feature data\n#     train_feats = train_feats.merge(Sentence_Eng(tmp), on='student_id', how='left')\n\n#     feature_names = list(filter(lambda x: x not in ['student_id','content', 'wording'], train_feats.columns))\n#     print('Features Number: ',len(feature_names))\n    \n#     # Word features\n#     tmp = Word_Preprocess(data)\n#     # Merge the newly generated feature data with the previously generated feature data\n#     train_feats = train_feats.merge(Word_Eng(tmp), on='student_id', how='left')\n\n#     feature_names = list(filter(lambda x: x not in ['student_id', 'content', 'wording'], train_feats.columns))\n#     print('Features Number: ',len(feature_names))\n    \n#     # TfidfVectorizer parameter\n#     if vectorizer == None:\n#         vectorizer = TfidfVectorizer(\n#                     tokenizer=lambda x: x,\n#                     preprocessor=lambda x: x,\n#                     token_pattern=None,\n#                     strip_accents='unicode',\n#                     analyzer = 'word',\n#                     ngram_range=(3,6),\n#                     min_df=0.05,\n#                     max_df=0.95,\n#                     sublinear_tf=True,\n#         )\n\n#         # Fit all datasets into TfidfVector,this may cause leakage and overly optimistic CV scores\n#         train_tfid = vectorizer.fit_transform([i for i in data['text']])\n#     else:\n#         train_tfid = vectorizer.transform([i for i in data['text']])\n        \n#     # Convert to array\n#     dense_matrix = train_tfid.toarray()\n#     # Convert to dataframe\n#     df = pd.DataFrame(dense_matrix)\n#     # rename features\n#     tfid_columns = [ f'tfid_{i}' for i in range(len(df.columns))]\n#     df.columns = tfid_columns\n#     df['student_id'] = train_feats['student_id']\n#     # Merge the newly generated feature data with the previously generated feature data\n#     train_feats = train_feats.merge(df, on='student_id', how='left')\n\n#     feature_names = list(filter(lambda x: x not in ['student_id','content', 'wording'], train_feats.columns))\n#     print('Features Number: ',len(feature_names))\n    \n#     if vectorizer_cnt == None:\n        \n#         vectorizer_cnt = CountVectorizer(\n#                 tokenizer=lambda x: x,\n#                 preprocessor=lambda x: x,\n#                 token_pattern=None,\n#                 strip_accents='unicode',\n#                 analyzer = 'word',\n#                 ngram_range=(2,3),\n#                 min_df=0.10,\n#                 max_df=0.85,\n#         )\n\n#         train_tfid = vectorizer_cnt.fit_transform([i for i in data['text']])\n#     else:\n#         train_tfid = vectorizer_cnt.transform([i for i in data['text']])\n        \n#     dense_matrix = train_tfid.toarray()\n#     df = pd.DataFrame(dense_matrix)\n#     tfid_columns = [ f'tfid_cnt_{i}' for i in range(len(df.columns))]\n#     df.columns = tfid_columns\n#     df['student_id'] = train_feats['student_id']\n#     train_feats = train_feats.merge(df, on='student_id', how='left')\n\n#     feature_names = list(filter(lambda x: x not in ['student_id','content', 'wording'], train_feats.columns))\n#     print('Features Number: ',len(feature_names))\n    \n#     # DeBERTa model predictions as features\n#     # TODO\n#     # deberta_oof = joblib.load('/kaggle/input/model_predictions.pkl')\n#     # print(deberta_oof.shape, train_feats.shape)\n\n#     # train_feats['deberta_oof'] = deberta_oof\n\n#     # feature_names = list(filter(lambda x: x not in ['student_id','content', 'wording'], train_feats.columns))\n#     # print('Features Number: ', len(feature_names))   \n\n#     # merge features \n#     if fold_indexes is not None:\n#         ft = features_train.iloc[fold_indexes].drop(columns=train.columns)\n#         train_feats_pandas = pd.DataFrame(train_feats.to_numpy(), columns=train_feats.columns, index=fold_indexes)\n#     else:\n#         ft = features_train.drop(columns=train.columns)\n#         train_feats_pandas = pd.DataFrame(train_feats.to_numpy(), columns=train_feats.columns)\n#     lgbm_input = pd.concat([train_feats_pandas, ft], axis=1)\n    \n#     feature_names = list(filter(lambda x: x not in ['student_id','content', 'wording'], lgbm_input.columns))\n#     print('Features Number: ',len(feature_names))\n    \n#     return lgbm_input[feature_names].astype(float), feature_names, vectorizer, vectorizer_cnt\n\n\n# # The loss function\n# def mcrmse(y_true, y_pred):\n#     assert y_true.shape == y_pred.shape\n#     scores = []\n#     for i in range(y_true.shape[1]):\n#         scores.append(mean_squared_error(y_true[:, i], y_pred[:, i], squared=False))\n#     return np.mean(scores), scores\n\n\n\n# # Randomized search\n# from sklearn.model_selection import RandomizedSearchCV\n\n# X = train_polars.drop(columns=['content', 'wording'])\n# y = train[['content', 'wording']].astype('float16')\n# X, _, _, _ = preprocess_data(X)\n\n# # Define the parameter grid for randomized search\n# param_distributions = {\n#     'learning_rate': [0.5, 0.01, 0.05, 0.1, 0.2, 0.3, 0.02, 0.03],\n#     'max_depth': [3, 4, 5, 6],\n#     'num_leaves': [5, 10, 15, 20, 25, 30, 35, 40],\n#     'colsample_bytree': [0.1, 0.3, 0.5, 0.7, 0.8],\n#     'reg_alpha': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 1.0],\n#     'reg_lambda': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 1.0],\n#     'n_estimators': [100, 300, 500, 700, 1000, 1500, 2000]\n# }\n\n# best_params = []\n# for target in ['content', 'wording']:\n    \n#     gkf = GroupKFold(n_splits=4)\n#     folds = gkf.split(X, y, groups=train['prompt_id'])\n\n#     # Initialize the LightGBM regressor\n#     model = lgb.LGBMRegressor(\n#             objective = 'regression',\n#             metrics = 'rmse',\n#             learning_rate = param_distributions['learning_rate'],\n#             max_depth =param_distributions['max_depth'],\n#             num_leaves =param_distributions['num_leaves'],\n#             colsample_bytree=param_distributions['colsample_bytree'],\n#             reg_alpha =param_distributions['reg_alpha'],\n#             reg_lambda =param_distributions['reg_lambda'],\n#             n_estimators=param_distributions['n_estimators'],\n#             random_state= random_seed,\n#             extra_trees=True,\n#             verbosity = -1)\n\n#     # Initialize RandomizedSearchCV\n#     random_search = RandomizedSearchCV(\n#         estimator=model,\n#         param_distributions=param_distributions,\n#         n_iter=70,\n#         scoring='neg_root_mean_squared_error',\n#         cv=folds,\n#         verbose=2,\n#         random_state=random_seed,\n#         n_jobs=-1\n#     )\n    \n#     # Fit the randomized search\n#     random_search.fit(X, y[target])\n#     best_params.append((random_search.best_score_, random_search.best_params_))\n#     print()\n    \n#     # Output the best parameters and best score for the current target\n#     print(f\"Best parameters found for {target}: \", random_search.best_params_)\n#     print(f\"Best RMSE score for {target}: \", -random_search.best_score_)\n#     print()\n    \n#     # breaking to find hyperparams only for content LGBM\n#     break\n    \n# print(\"Content best params and scores:\", best_params[0])\n# # print(\"Wording best params and scores:\", best_params[1])\n\n\n# def feature_select_wrapper():\n#     \"\"\"\n#     lgm\n#     :param train\n#     :param test\n#     :return\n#     \"\"\"\n    \n#     X = train_polars.drop(columns=['content', 'wording'])\n#     y = train[['content', 'wording']].astype('float16')\n#     gkf = GroupKFold(n_splits=4)\n#     folds = gkf.split(X, y, groups=train['prompt_id'])\n#     callbacks = [log_evaluation(period=25), early_stopping(stopping_rounds=700, first_metric_only=True)]\n\n#     # Part 1.\n#     print('feature_select_wrapper...')\n    \n#     models = []\n#     predictions = []\n#     fold_scores = []\n#     fse_content = {}\n#     fse_wording = {}\n#     mcrmse_scores = np.array([])\n    \n#     for fold_idx, (train_index, test_index) in enumerate(folds):\n\n#         print('fold', fold_idx)\n#         X_train_fold, X_test_fold = X[train_index], X[test_index]\n#         y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index]\n        \n#         X_train_fold, feature_names, vectorizer, vectorizer_cnt = preprocess_data(X_train_fold, fold_indexes=train_index)\n#         X_test_fold, _, _, _ = preprocess_data(X_test_fold, fold_indexes=test_index, vectorizer=vectorizer, vectorizer_cnt=vectorizer_cnt)\n        \n#         features = feature_names \n        \n#         fold_predictions = []\n#         fold_models = []\n#         for target in ['content', 'wording']:\n#             y_train_target = y_train_fold[target]\n#             y_test_target = y_test_fold[target]\n            \n#             model = lgb.LGBMRegressor(\n#                     objective = 'regression',\n#                     metrics = 'rmse',\n#                     learning_rate = 0.01,\n#                     max_depth = 3,\n#                     num_leaves = 10,\n#                     colsample_bytree=0.7,\n#                     reg_alpha = 0.1,\n#                     reg_lambda = 1.0,\n#                     n_estimators=100,\n#                     random_state= random_seed,\n#                     extra_trees=True,\n#                     verbosity = -1)\n\n#             predictor = model.fit(X_train_fold,\n#                                   y_train_target,\n#                                   eval_names=['train', 'valid'],\n#                                   eval_set=[(X_train_fold, y_train_target), (X_test_fold, y_test_target)],\n#                                   eval_metric='rmse',\n#                                   callbacks=callbacks)\n            \n#             fold_models.append(predictor)\n#             fold_predictions.append(predictor.predict(X_test_fold))\n            \n#             # Aggregate feature importances\n#             importances = pd.Series(predictor.feature_importances_, index=features)\n#             print()\n#             if target == 'content':\n#                 for feature, importance in importances.items():\n#                     if feature in fse_content:\n#                         fse_content[feature] += importance\n#                     else:\n#                         fse_content[feature] = importance\n#             else:\n#                 for feature, importance in importances.items():\n#                     if feature in fse_wording:\n#                         fse_wording[feature] += importance\n#                     else:\n#                         fse_wording[feature] = importance\n                \n#         models.append(fold_models)\n#         predictions.append(fold_predictions)\n        \n#         # Calculate mcrmse \n#         y_true_fold = np.array(y_test_fold)\n#         y_pred_fold = np.array(fold_predictions).T.reshape(-1, 2)\n#         fold_mcrmse, _ = mcrmse(y_true_fold, y_pred_fold)    \n#         mcrmse_scores = np.append(mcrmse_scores, fold_mcrmse)\n#         print(f\"Fold {fold_idx} MCRMSE: {fold_mcrmse}\")\n#         print()\n    \n#     print(f\"MCRMSE across all folds: {np.mean(mcrmse_scores)}\")\n    \n#     # Convert dictionaries to Series\n#     fse_content = pd.Series(fse_content)\n#     fse_wording = pd.Series(fse_wording)\n#     fse_aggregate = (fse_content + fse_wording) / 2\n    \n#     # Part 4.\n#     feature_select_content = fse_content.sort_values(ascending=False).index.tolist()\n#     feature_select_wording = fse_wording.sort_values(ascending=False).index.tolist()\n#     feature_select_aggregate = fse_aggregate.sort_values(ascending=False).index.tolist()\n\n#     # Combine feature importance scores into a DataFrame\n#     feature_importance_df = pd.DataFrame({\n#         'Feature':  fse_aggregate.index,\n#         'Content_Importance': fse_content,\n#         'Wording_Importance': fse_wording,\n#         'Aggregate_Importance': fse_aggregate\n#     }).fillna(0)\n    \n#     # Sort by aggregate importance and select top features\n#     feature_importance_df = feature_importance_df.sort_values(by='Aggregate_Importance', ascending=False).reset_index(drop=True)\n#     top_features = feature_importance_df\n    \n#     print('done')\n#     return top_features, feature_select_content, feature_select_wording, feature_select_aggregate\n\n# # Run feature selection\n# top_features, fs_content, fs_wording, fs_aggregate = feature_select_wrapper()\n\n\n# # select num of features based on the plots\n# # Plot the data\n# plt.figure(figsize=(12, 10))\n# top_idxs = 30\n\n# # plot options: Content_Importance, Wording_Importance, Aggregate_Importance\n# importance_to_plot = 'Aggregate_Importance'\n# plt.barh(top_features['Feature'][:top_idxs], top_features.sort_values(by=importance_to_plot, ascending=False).reset_index(drop=True)[importance_to_plot][:top_idxs], color='skyblue')\n# plt.xlabel('Feature importance')\n# plt.ylabel('Features')\n# plt.title('Feature Importance')\n# plt.gca().invert_yaxis()  # Invert y-axis to have the highest importance on top\n# plt.show()\n\n# # fs_content[:top_idxs]\n# # fs_wording[:top_idxs]\n# # fs_aggregate[:top_idxs]\n# top_features.sort_values(by=importance_to_plot, ascending=False).reset_index(drop=True)[:top_idxs]\n\n\n# LOAD = True # re-train\n# # Define the number of splits for cross-validation\n# n_splits = 4\n# models = []\n# if not LOAD:\n#     for i in range(n_splits):\n#         # TODO: Change path !!!!!!!\n#         models.append(lgb.Booster(model_file=f'/kaggle/input/aes-15fold/fold_{i+1}.txt'))\n# else:\n#     X = train_polars.drop(columns=['content', 'wording'])\n#     y = train[['content', 'wording']].astype('float16')\n#     gkf = GroupKFold(n_splits=4)\n#     folds = gkf.split(X, y, groups=train['prompt_id'])\n#     callbacks = [log_evaluation(period=25), early_stopping(stopping_rounds=700, first_metric_only=True)]\n#     predictions = []\n#     mcrmse_scores = np.array([])\n    \n#     top_features_list = top_features['Feature'][:top_idxs]\n#     # Loop through each fold of the cross-validation\n#     for fold_idx, (train_index, test_index) in enumerate(folds):\n        \n#         print('fold', fold_idx)\n#         X_train_fold, X_test_fold = X[train_index], X[test_index]\n#         y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index]\n        \n#         X_train_fold, feature_names, vectorizer, vectorizer_cnt = preprocess_data(X_train_fold, fold_indexes=train_index)\n#         X_test_fold, _, _, _ = preprocess_data(X_test_fold, fold_indexes=test_index, vectorizer=vectorizer, vectorizer_cnt=vectorizer_cnt)\n        \n#         # Intersect the list of top features with the columns in X_train_fold\n#         existing_top_features = [feature for feature in top_features_list if feature in X_train_fold.columns]\n        \n#         # Using the top features only\n#         X_train_fold = X_train_fold[existing_top_features]\n#         X_test_fold = X_test_fold[existing_top_features]\n        \n#         print(X_train_fold.shape)\n        \n#         fold_predictions = []\n#         fold_models = []\n        \n#         for target in ['content', 'wording']:\n#             y_train_target = y_train_fold[target]\n#             y_test_target = y_test_fold[target]\n            \n#             model = lgb.LGBMRegressor(\n#                     objective = 'regression',\n#                     metrics = 'rmse',\n#                     learning_rate = 0.01,\n#                     max_depth = 3,\n#                     num_leaves = 10,\n#                     colsample_bytree=0.7,\n#                     reg_alpha = 0.1,\n#                     reg_lambda = 1.0,\n#                     n_estimators=100,\n#                     random_state= random_seed,\n#                     extra_trees=True,\n#                     verbosity = -1)\n\n#             predictor = model.fit(X_train_fold,\n#                                   y_train_target,\n#                                   eval_names=['train', 'valid'],\n#                                   eval_set=[(X_train_fold, y_train_target), (X_test_fold, y_test_target)],\n#                                   eval_metric='rmse',\n#                                   callbacks=callbacks)\n            \n#             fold_models.append(predictor)\n#             fold_predictions.append(predictor.predict(X_test_fold))\n#             # predictor.booster_.save_model(f'fold_{fold_idx}_{target}.txt')\n            \n#         models.append(fold_models)\n#         predictions.append(fold_predictions)\n        \n#         # Calculate mcrmse \n#         y_true_fold = np.array(y_test_fold)\n#         y_pred_fold = np.array(fold_predictions).T.reshape(-1, 2)\n#         fold_mcrmse, _ = mcrmse(y_true_fold, y_pred_fold)    \n#         mcrmse_scores = np.append(mcrmse_scores, fold_mcrmse)\n#         print(f\"Fold {fold_idx} MCRMSE: {fold_mcrmse}\")\n#         print()\n        \n#     print(f\"MCRMSE across all folds: {np.mean(mcrmse_scores)}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Ensemble of Seeds","metadata":{}},{"cell_type":"code","source":"\n#  preds = []\n#  for i in range(NUM_OF_FOLDS):\n#      if i == 0:\n#          model.load_weights(f'/kaggle/input/no-aug-full-model-4epochs/no_augment_full_model_4epochs..weights.h5')\n#      else:\n#          model, _ = create_model(decay_steps=decay_steps)\n#          model.load_weights(f'/kaggle/input/no-aug-full-model-4epochs/no_augment_full_model_seed_0.weights.h5')  \n#      ids, contents, wordings = generate_predictions(model, test_data)\n#      preds.append([ids, contents, wordings])\n#      del model\n#      gc.collect()\n\n#  # Calculate mean predictions\n#  contents = np.stack([pred[1] for pred in preds]).mean(axis=0)\n#  wordings = np.stack([pred[2] for pred in preds]).mean(axis=0)","metadata":{},"execution_count":null,"outputs":[]}]}