{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8870053,"sourceType":"datasetVersion","datasetId":5307916},{"sourceId":2977194,"sourceType":"datasetVersion","datasetId":1825054},{"sourceId":8558953,"sourceType":"datasetVersion","datasetId":5115575}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"# General imports\nimport numpy as np\nimport pandas as pd\nfrom IPython.display import display\nimport random\nimport shutil\nimport os\n\n# Neural network imports\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras import layers\nfrom keras.callbacks import EarlyStopping\nfrom transformers import TFAutoModel, AutoTokenizer\nfrom sklearn.model_selection import GroupKFold\n!pip install /kaggle/input/autocorrect/autocorrect-2.6.1.tar\nfrom autocorrect import Speller\n\n# Initializing autocorrect\nspell = Speller(lang='en', fast=True)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    epochs=12\n    pre_trained_model_name=\"/kaggle/input/deberta-v3-large/deberta_v3_large/\"\n    final_model_path = f'full_model_scaled-{epochs}.keras'\n    learning_rate=0.00015\n    weight_decay=1e-4\n    warmup_steps=100\n    hidden_dropout_prob=0.\n    attention_probs_dropout_prob=0.\n    n_splits=4\n    batch_size=4\n    random_seed=42\n    max_length=1575\n    embeddings_len=1024","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(random_seed):\n    \n    os.environ['PYTHONHASHSEED'] = str(random_seed)\n    np.random.seed(random_seed)\n    tf.random.set_seed(random_seed)\n    random.seed(random_seed)\n    keras.utils.set_random_seed(random_seed)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def move_to_working_folder(source_path, destination_path):\n    shutil.copy(source_path, destination_path)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocess Input","metadata":{}},{"cell_type":"code","source":"prefix1 = \"Think through this step by step: \"\nprefix2 = \"Pay attention to the content and wording: \"\n\n# This function creates input ids, attention mask and head mask\ndef preprocess(summary, prompt_question, prompt_text, tokenizer, is_demo=False):\n    \n    sep = f\" {tokenizer.sep_token} \" \n    \n    if is_demo:\n        summary = prefix1 + prompt_question + sep + prefix2 + spell(summary) + sep + prompt_text\n        tokenized = tokenizer(summary,\n                              add_special_tokens=False,\n                              truncation=True,\n                              padding='max_length',\n                              return_tensors='tf',\n                              max_length=CFG.max_length,\n                              return_attention_mask=True)\n    else:\n        summary = prefix1 + prompt_question + sep + prefix2 + summary.apply(spell) + sep + prompt_text\n        tokenized = tokenizer.batch_encode_plus(summary.tolist(),\n                                                add_special_tokens=False,\n                                                truncation=True,\n                                                padding='max_length',\n                                                return_tensors='tf',\n                                                max_length=CFG.max_length,\n                                                return_attention_mask=True)\n    \n    input_ids = tokenized['input_ids']\n    attention_mask = tokenized['attention_mask']\n\n    # Create head mask\n    head_mask = np.zeros(input_ids.shape)\n    for i, summ in enumerate(input_ids.numpy()):\n        use_full = False\n        for j, token in enumerate(summ):\n            if token == tokenizer.sep_token_id:\n                use_full = not use_full  \n            elif token == tokenizer.pad_token_id:\n                break\n            head_mask[i][j] = (1. if use_full else 0.) \n    return [input_ids.numpy(), attention_mask.numpy(), head_mask.astype(np.float16)]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Define Model","metadata":{}},{"cell_type":"code","source":"# Creates a model that wraps the pre trained model\n@keras.utils.register_keras_serializable()\nclass PreTrainedModel(keras.Model):\n    def __init__(self, model_path, trainable=False, num_layers_to_freeze=0, name=None, **kwargs):\n        super().__init__(name=name, **kwargs)\n        self.model_path = model_path\n        self.trainable = trainable\n        self.num_layers_to_freeze = num_layers_to_freeze\n        \n        # Load model and tokenizer\n        self.model = TFAutoModel.from_pretrained(model_path + \"model\") \n        self.tokenizer = AutoTokenizer.from_pretrained(model_path + \"tokenizer\")\n        \n        # Define model configurations\n        self.model.trainable = self.trainable\n        self.model.config.hidden_dropout_prob = CFG.hidden_dropout_prob\n        self.model.config.attention_probs_dropout_prob = CFG.attention_probs_dropout_prob\n        \n        # Freeze layers if trainable\n        if self.trainable:\n            self.model.trainable = self.trainable\n            if self.trainable:\n                for layer in self.model.layers[0].encoder.layer[:self.num_layers_to_freeze]:\n                    layer.trainable = False\n\n    # Call the pre trained model and get the all hidden state\n    def call(self, input_ids, attention_mask):\n        output = self.model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)\n        return output.hidden_states\n    \n    def get_config(self):\n        config = super().get_config().copy()\n        config.update({\n            'model_path': self.model_path,\n            'trainable': self.trainable,\n            'num_layers_to_freeze': self.num_layers_to_freeze\n        })\n        return config\n\n    @classmethod\n    def from_config(cls, config):\n        return cls(**config)\n    \n@keras.utils.register_keras_serializable()\ndef build_deberta():\n    return PreTrainedModel(CFG.pre_trained_model_name, name=\"deberta_layer\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define layers for head mask step\n\n@keras.utils.register_keras_serializable()\nclass ExpandDimsLayer(layers.Layer):\n    def __init__(self, **kwargs):\n        super(ExpandDimsLayer, self).__init__(**kwargs)\n\n    def call(self, inputs):\n        return tf.expand_dims(tf.cast(inputs, dtype=tf.float32), axis=-1)\n\n@keras.utils.register_keras_serializable()\nclass MaskedEmbeddingsLayer(layers.Layer):\n    def __init__(self, **kwargs):\n        super(MaskedEmbeddingsLayer, self).__init__(**kwargs)\n\n    def call(self, inputs):\n        hidden_state, h_mask = inputs\n        return tf.multiply(hidden_state, h_mask)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Define Loss Function","metadata":{}},{"cell_type":"code","source":"# The loss function\n@keras.utils.register_keras_serializable()\ndef mcrmse(y_true, y_pred):\n    y_true = tf.cast(y_true, tf.float16)\n    y_pred = tf.cast(y_pred, tf.float16)\n    columnwise_mse = tf.reduce_mean(tf.square(y_true - y_pred), axis=0)\n    return tf.reduce_mean(tf.sqrt(columnwise_mse), axis=-1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Generate Predictions","metadata":{}},{"cell_type":"code","source":"def generate_predictions(model, data):\n    contents = []\n    wordings = []\n    ids = []\n    predictions = model.predict(x=[data['input_ids'], data['attention_mask'], data['head_mask']],\n                                batch_size=CFG.batch_size)\n\n    for idx, output in enumerate(predictions):\n        contents.append(output[0])\n        wordings.append(output[1])\n        ids.append(data['student_id'][idx])\n\n    contents = np.exp(contents) - 3\n    wordings = np.exp(wordings) - 3\n        \n    return ids, contents, wordings","metadata":{},"execution_count":null,"outputs":[]}]}